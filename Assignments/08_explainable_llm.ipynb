{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mille055/AIPI590-XAI/blob/main/Assignments/08_explainable_llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2OkAhQG2V8n"
      },
      "source": [
        "<a href='https://ai.meng.duke.edu'> = <img align=\"left\" style=\"padding-top:10px;\" src=https://storage.googleapis.com/aipi_datasets/Duke-AIPI-Logo.png>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AIPI 590 - XAI | Assignment 08\n",
        "\n",
        "#Description: Interpretable LLM\n",
        "This notebook is for exploring explainability of LLMs.\n",
        "\n",
        "\n",
        "## Chad Miller\n",
        "\n",
        "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mille055/AIPI590-XAI/blob/main/Assignments/08_explainable_llm.ipynb)"
      ],
      "metadata": {
        "id": "BOiHozMSBh28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Acknowledgements: Class Repository code on GAM model, kaggle telco-customer-churn dataset"
      ],
      "metadata": {
        "id": "5QIlTW-WBrTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Remove Colab default sample_data\n",
        "!rm -r /content/sample_data\n",
        "\n",
        "# Clone GitHub files to colab workspace\n",
        "repo_name = \"AIPI590-XAI\"\n",
        "git_path = 'https://github.com/mille055/AIPI590-XAI.git'\n",
        "!git clone \"{git_path}\"\n",
        "# !git clone 'https://github.com/mille055/CT_Protocol.git'\n",
        "\n",
        "# Install dependencies from requirements.txt file\n",
        "!pip install -r \"{os.path.join(repo_name,'requirements.txt')}\"\n",
        "# !pip install -r \"{os.path.join(repo_name, 'requirements2.txt')}\"\n",
        "\n",
        "notebook_dir = 'Assignments'\n",
        "path_to_notebook = os.path.join(repo_name,notebook_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uD5YGTd3Kin0",
        "outputId": "5c6d8e19-76ac-4501-ca65-6be341424eed"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/sample_data': No such file or directory\n",
            "fatal: destination path 'AIPI590-XAI' already exists and is not an empty directory.\n",
            "Collecting rulefit@ git+https://github.com/christophM/rulefit.git (from -r AIPI590-XAI/requirements.txt (line 14))\n",
            "  Cloning https://github.com/christophM/rulefit.git to /tmp/pip-install-ad73onf5/rulefit_a5f61073db924d2eae104e9cf71159ae\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/christophM/rulefit.git /tmp/pip-install-ad73onf5/rulefit_a5f61073db924d2eae104e9cf71159ae\n",
            "  Resolved https://github.com/christophM/rulefit.git to commit 472b8574b4eb9e565caf1e05ed580998fe2c9a8e\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting alepython@ git+https://github.com/MaximeJumelle/ALEPython.git (from -r AIPI590-XAI/requirements.txt (line 15))\n",
            "  Cloning https://github.com/MaximeJumelle/ALEPython.git to /tmp/pip-install-ad73onf5/alepython_69bb84ce5aec41a08b5134388ca71d92\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/MaximeJumelle/ALEPython.git /tmp/pip-install-ad73onf5/alepython_69bb84ce5aec41a08b5134388ca71d92\n",
            "  Resolved https://github.com/MaximeJumelle/ALEPython.git to commit 286350ab674980a32270db2a0b5ccca1380312a7\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r AIPI590-XAI/requirements.txt (line 1)) (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from -r AIPI590-XAI/requirements.txt (line 2)) (0.20.0+cu121)\n",
            "Requirement already satisfied: adversarial-robustness-toolbox in /usr/local/lib/python3.10/dist-packages (from -r AIPI590-XAI/requirements.txt (line 3)) (1.18.2)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from -r AIPI590-XAI/requirements.txt (line 4)) (1.6.17)\n",
            "Requirement already satisfied: pygam in /usr/local/lib/python3.10/dist-packages (from -r AIPI590-XAI/requirements.txt (line 5)) (0.9.1)\n",
            "Requirement already satisfied: imodels in /usr/local/lib/python3.10/dist-packages (from -r AIPI590-XAI/requirements.txt (line 6)) (2.0.0)\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.10/dist-packages (from -r AIPI590-XAI/requirements.txt (line 7)) (0.46.0)\n",
            "Requirement already satisfied: lime in /usr/local/lib/python3.10/dist-packages (from -r AIPI590-XAI/requirements.txt (line 8)) (0.2.0.1)\n",
            "Requirement already satisfied: alibi in /usr/local/lib/python3.10/dist-packages (from -r AIPI590-XAI/requirements.txt (line 9)) (0.9.6)\n",
            "Requirement already satisfied: pydicom==2.1.2 in /usr/local/lib/python3.10/dist-packages (from -r AIPI590-XAI/requirements.txt (line 10)) (2.1.2)\n",
            "Requirement already satisfied: pdpbox in /usr/local/lib/python3.10/dist-packages (from -r AIPI590-XAI/requirements.txt (line 11)) (0.3.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from -r AIPI590-XAI/requirements.txt (line 12)) (0.24.7)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from -r AIPI590-XAI/requirements.txt (line 13)) (3.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r AIPI590-XAI/requirements.txt (line 1)) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r AIPI590-XAI/requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r AIPI590-XAI/requirements.txt (line 1)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r AIPI590-XAI/requirements.txt (line 1)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r AIPI590-XAI/requirements.txt (line 1)) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->-r AIPI590-XAI/requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->-r AIPI590-XAI/requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->-r AIPI590-XAI/requirements.txt (line 2)) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->-r AIPI590-XAI/requirements.txt (line 2)) (10.4.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox->-r AIPI590-XAI/requirements.txt (line 3)) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox->-r AIPI590-XAI/requirements.txt (line 3)) (1.5.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox->-r AIPI590-XAI/requirements.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox->-r AIPI590-XAI/requirements.txt (line 3)) (75.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox->-r AIPI590-XAI/requirements.txt (line 3)) (4.66.5)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle->-r AIPI590-XAI/requirements.txt (line 4)) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle->-r AIPI590-XAI/requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle->-r AIPI590-XAI/requirements.txt (line 4)) (2.32.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->-r AIPI590-XAI/requirements.txt (line 4)) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->-r AIPI590-XAI/requirements.txt (line 4)) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->-r AIPI590-XAI/requirements.txt (line 4)) (6.1.0)\n",
            "Requirement already satisfied: progressbar2<5.0.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pygam->-r AIPI590-XAI/requirements.txt (line 5)) (4.5.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from imodels->-r AIPI590-XAI/requirements.txt (line 6)) (3.7.1)\n",
            "Requirement already satisfied: mlxtend>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from imodels->-r AIPI590-XAI/requirements.txt (line 6)) (0.23.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from imodels->-r AIPI590-XAI/requirements.txt (line 6)) (2.2.2)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap->-r AIPI590-XAI/requirements.txt (line 7)) (24.1)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.10/dist-packages (from shap->-r AIPI590-XAI/requirements.txt (line 7)) (0.0.8)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap->-r AIPI590-XAI/requirements.txt (line 7)) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap->-r AIPI590-XAI/requirements.txt (line 7)) (3.1.0)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.10/dist-packages (from lime->-r AIPI590-XAI/requirements.txt (line 8)) (0.22.0)\n",
            "Requirement already satisfied: spacy<4.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi->-r AIPI590-XAI/requirements.txt (line 9)) (3.7.5)\n",
            "Requirement already satisfied: blis<0.8.0 in /usr/local/lib/python3.10/dist-packages (from alibi->-r AIPI590-XAI/requirements.txt (line 9)) (0.7.11)\n",
            "Requirement already satisfied: attrs<24.0.0,>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from alibi->-r AIPI590-XAI/requirements.txt (line 9)) (23.2.0)\n",
            "Requirement already satisfied: dill<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from alibi->-r AIPI590-XAI/requirements.txt (line 9)) (0.3.8)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from alibi->-r AIPI590-XAI/requirements.txt (line 9)) (4.44.2)\n",
            "Requirement already satisfied: joblib>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from pdpbox->-r AIPI590-XAI/requirements.txt (line 11)) (1.4.2)\n",
            "Requirement already satisfied: plotly>=5.9.0 in /usr/local/lib/python3.10/dist-packages (from pdpbox->-r AIPI590-XAI/requirements.txt (line 11)) (5.24.1)\n",
            "Requirement already satisfied: pqdm>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from pdpbox->-r AIPI590-XAI/requirements.txt (line 11)) (0.2.0)\n",
            "Requirement already satisfied: psutil>=5.9.0 in /usr/local/lib/python3.10/dist-packages (from pdpbox->-r AIPI590-XAI/requirements.txt (line 11)) (5.9.5)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from pdpbox->-r AIPI590-XAI/requirements.txt (line 11)) (7.4.4)\n",
            "Requirement already satisfied: sphinx>=5.0.2 in /usr/local/lib/python3.10/dist-packages (from pdpbox->-r AIPI590-XAI/requirements.txt (line 11)) (8.1.3)\n",
            "Requirement already satisfied: sphinx-rtd-theme>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from pdpbox->-r AIPI590-XAI/requirements.txt (line 11)) (3.0.1)\n",
            "Requirement already satisfied: numpydoc>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from pdpbox->-r AIPI590-XAI/requirements.txt (line 11)) (1.8.0)\n",
            "Requirement already satisfied: xgboost>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from pdpbox->-r AIPI590-XAI/requirements.txt (line 11)) (2.1.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->-r AIPI590-XAI/requirements.txt (line 12)) (6.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r AIPI590-XAI/requirements.txt (line 13)) (16.1.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->-r AIPI590-XAI/requirements.txt (line 13)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->-r AIPI590-XAI/requirements.txt (line 13)) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->-r AIPI590-XAI/requirements.txt (line 13)) (3.10.10)\n",
            "Requirement already satisfied: ordered-set>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from rulefit@ git+https://github.com/christophM/rulefit.git->-r AIPI590-XAI/requirements.txt (line 14)) (4.1.0)\n",
            "Requirement already satisfied: loguru>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from alepython@ git+https://github.com/MaximeJumelle/ALEPython.git->-r AIPI590-XAI/requirements.txt (line 15)) (0.7.2)\n",
            "Requirement already satisfied: seaborn>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from alepython@ git+https://github.com/MaximeJumelle/ALEPython.git->-r AIPI590-XAI/requirements.txt (line 15)) (0.13.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r AIPI590-XAI/requirements.txt (line 13)) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r AIPI590-XAI/requirements.txt (line 13)) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r AIPI590-XAI/requirements.txt (line 13)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r AIPI590-XAI/requirements.txt (line 13)) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r AIPI590-XAI/requirements.txt (line 13)) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r AIPI590-XAI/requirements.txt (line 13)) (4.0.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imodels->-r AIPI590-XAI/requirements.txt (line 6)) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imodels->-r AIPI590-XAI/requirements.txt (line 6)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imodels->-r AIPI590-XAI/requirements.txt (line 6)) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imodels->-r AIPI590-XAI/requirements.txt (line 6)) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imodels->-r AIPI590-XAI/requirements.txt (line 6)) (3.2.0)\n",
            "Requirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.10/dist-packages (from numpydoc>=1.4.0->pdpbox->-r AIPI590-XAI/requirements.txt (line 11)) (0.9.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from numpydoc>=1.4.0->pdpbox->-r AIPI590-XAI/requirements.txt (line 11)) (2.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->imodels->-r AIPI590-XAI/requirements.txt (line 6)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->imodels->-r AIPI590-XAI/requirements.txt (line 6)) (2024.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=5.9.0->pdpbox->-r AIPI590-XAI/requirements.txt (line 11)) (9.0.0)\n",
            "Requirement already satisfied: bounded-pool-executor in /usr/local/lib/python3.10/dist-packages (from pqdm>=0.2.0->pdpbox->-r AIPI590-XAI/requirements.txt (line 11)) (0.0.3)\n",
            "Requirement already satisfied: python-utils>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from progressbar2<5.0.0,>=4.2.0->pygam->-r AIPI590-XAI/requirements.txt (line 5)) (3.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->-r AIPI590-XAI/requirements.txt (line 4)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->-r AIPI590-XAI/requirements.txt (line 4)) (3.10)\n",
            "Requirement already satisfied: imageio>=2.27 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime->-r AIPI590-XAI/requirements.txt (line 8)) (2.35.1)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime->-r AIPI590-XAI/requirements.txt (line 8)) (2024.9.20)\n",
            "Requirement already satisfied: lazy_loader>=0.3 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime->-r AIPI590-XAI/requirements.txt (line 8)) (0.4)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->adversarial-robustness-toolbox->-r AIPI590-XAI/requirements.txt (line 3)) (3.5.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi->-r AIPI590-XAI/requirements.txt (line 9)) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi->-r AIPI590-XAI/requirements.txt (line 9)) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi->-r AIPI590-XAI/requirements.txt (line 9)) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi->-r AIPI590-XAI/requirements.txt (line 9)) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi->-r AIPI590-XAI/requirements.txt (line 9)) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi->-r AIPI590-XAI/requirements.txt (line 9)) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi->-r AIPI590-XAI/requirements.txt (line 9)) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi->-r AIPI590-XAI/requirements.txt (line 9)) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi->-r AIPI590-XAI/requirements.txt (line 9)) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi->-r AIPI590-XAI/requirements.txt (line 9)) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi->-r AIPI590-XAI/requirements.txt (line 9)) (0.12.5)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi->-r AIPI590-XAI/requirements.txt (line 9)) (2.9.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi->-r AIPI590-XAI/requirements.txt (line 9)) (3.4.1)\n",
            "Requirement already satisfied: spacy-lookups-data<1.1.0,>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi->-r AIPI590-XAI/requirements.txt (line 9)) (1.0.5)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.0.2->pdpbox->-r AIPI590-XAI/requirements.txt (line 11)) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.0.2->pdpbox->-r AIPI590-XAI/requirements.txt (line 11)) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.0.2->pdpbox->-r AIPI590-XAI/requirements.txt (line 11)) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.0.2->pdpbox->-r AIPI590-XAI/requirements.txt (line 11)) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.0.2->pdpbox->-r AIPI590-XAI/requirements.txt (line 11)) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.0.2->pdpbox->-r AIPI590-XAI/requirements.txt (line 11)) (2.0.0)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.0.2->pdpbox->-r AIPI590-XAI/requirements.txt (line 11)) (2.18.0)\n",
            "Requirement already satisfied: docutils<0.22,>=0.20 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.0.2->pdpbox->-r AIPI590-XAI/requirements.txt (line 11)) (0.21.2)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.0.2->pdpbox->-r AIPI590-XAI/requirements.txt (line 11)) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.0.2->pdpbox->-r AIPI590-XAI/requirements.txt (line 11)) (2.16.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.0.2->pdpbox->-r AIPI590-XAI/requirements.txt (line 11)) (0.7.16)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.0.2->pdpbox->-r AIPI590-XAI/requirements.txt (line 11)) (1.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r AIPI590-XAI/requirements.txt (line 1)) (3.0.2)\n",
            "Requirement already satisfied: sphinxcontrib-jquery<5,>=4 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme>=1.1.1->pdpbox->-r AIPI590-XAI/requirements.txt (line 11)) (4.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.7.0->alibi->-r AIPI590-XAI/requirements.txt (line 9)) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.7.0->alibi->-r AIPI590-XAI/requirements.txt (line 9)) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.7.0->alibi->-r AIPI590-XAI/requirements.txt (line 9)) (0.19.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost>=1.7.1->pdpbox->-r AIPI590-XAI/requirements.txt (line 11)) (2.23.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->-r AIPI590-XAI/requirements.txt (line 4)) (0.5.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap->-r AIPI590-XAI/requirements.txt (line 7)) (0.43.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->pdpbox->-r AIPI590-XAI/requirements.txt (line 11)) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->pdpbox->-r AIPI590-XAI/requirements.txt (line 11)) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->pdpbox->-r AIPI590-XAI/requirements.txt (line 11)) (1.2.2)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->-r AIPI590-XAI/requirements.txt (line 4)) (1.3)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi->-r AIPI590-XAI/requirements.txt (line 9)) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi->-r AIPI590-XAI/requirements.txt (line 9)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi->-r AIPI590-XAI/requirements.txt (line 9)) (2.23.4)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi->-r AIPI590-XAI/requirements.txt (line 9)) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi->-r AIPI590-XAI/requirements.txt (line 9)) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi->-r AIPI590-XAI/requirements.txt (line 9)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi->-r AIPI590-XAI/requirements.txt (line 9)) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi->-r AIPI590-XAI/requirements.txt (line 9)) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi->-r AIPI590-XAI/requirements.txt (line 9)) (7.0.5)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets->-r AIPI590-XAI/requirements.txt (line 13)) (0.2.0)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi->-r AIPI590-XAI/requirements.txt (line 9)) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi->-r AIPI590-XAI/requirements.txt (line 9)) (3.0.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi->-r AIPI590-XAI/requirements.txt (line 9)) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi->-r AIPI590-XAI/requirements.txt (line 9)) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Standard libraries\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import tabulate\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "\n",
        "## Imports for plotting\n",
        "from IPython.display import Image\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython.display import set_matplotlib_formats\n",
        "#set_matplotlib_formats('svg', 'pdf') # For export\n",
        "from matplotlib.colors import to_rgb\n",
        "import matplotlib\n",
        "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "\n",
        "## Progress bar\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "\n",
        "# Torchvision\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms, datasets\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageNet\n",
        "\n",
        "# Other\n",
        "from huggingface_hub import login\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from openai import OpenAI\n",
        "from getpass import getpass\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "VQo-GztgCHWl"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = None\n",
        "\n",
        "# Try to load the API key from an environment variable\n",
        "api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# If the API key is not found,\n",
        "if not api_key:\n",
        "    print('not in environment variables...')\n",
        "    api_key = getpass(\"Enter your OpenAI API key: \")\n",
        "\n",
        "if api_key:\n",
        "  print('API key found')\n",
        "\n",
        "\n",
        "\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85fehzhszobv",
        "outputId": "44250de1-c7b4-473e-c733-616e6fa4498e"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API key found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the eigth grade math dataset from Hugging Face Hub\n",
        "dataset = load_dataset(\"gsm8k\", 'main', split=\"train\")\n",
        "\n",
        "# making dataframe version\n",
        "df = dataset.to_pandas()\n",
        "# Dataset size\n",
        "print(f'Downloaded dataset length is ', len(dataset))\n",
        "\n",
        "\n",
        "# limiting to first 200\n",
        "print('Limiting to the first 200 Q A pairs for this analysis.')\n",
        "print('Some example question and answers:')\n",
        "df = df[:200]\n",
        "df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "iejV6tSojqCC",
        "outputId": "310f63cd-65ad-4971-be0b-c9a507db238f"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded dataset length is  7473\n",
            "Limiting to the first 200 Q A pairs for this analysis.\n",
            "Some example question and answers:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            question  \\\n",
              "0  Natalia sold clips to 48 of her friends in Apr...   \n",
              "1  Weng earns $12 an hour for babysitting. Yester...   \n",
              "2  Betty is saving money for a new wallet which c...   \n",
              "3  Julie is reading a 120-page book. Yesterday, s...   \n",
              "4  James writes a 3-page letter to 2 different fr...   \n",
              "\n",
              "                                              answer  \n",
              "0  Natalia sold 48/2 = <<48/2=24>>24 clips in May...  \n",
              "1  Weng earns 12/60 = $<<12/60=0.2>>0.2 per minut...  \n",
              "2  In the beginning, Betty has only 100 / 2 = $<<...  \n",
              "3  Maila read 12 x 2 = <<12*2=24>>24 pages today....  \n",
              "4  He writes each friend 3*2=<<3*2=6>>6 pages a w...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0d650424-0d20-497f-8d36-045c701aea54\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Natalia sold clips to 48 of her friends in Apr...</td>\n",
              "      <td>Natalia sold 48/2 = &lt;&lt;48/2=24&gt;&gt;24 clips in May...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Weng earns $12 an hour for babysitting. Yester...</td>\n",
              "      <td>Weng earns 12/60 = $&lt;&lt;12/60=0.2&gt;&gt;0.2 per minut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Betty is saving money for a new wallet which c...</td>\n",
              "      <td>In the beginning, Betty has only 100 / 2 = $&lt;&lt;...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Julie is reading a 120-page book. Yesterday, s...</td>\n",
              "      <td>Maila read 12 x 2 = &lt;&lt;12*2=24&gt;&gt;24 pages today....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>James writes a 3-page letter to 2 different fr...</td>\n",
              "      <td>He writes each friend 3*2=&lt;&lt;3*2=6&gt;&gt;6 pages a w...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0d650424-0d20-497f-8d36-045c701aea54')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0d650424-0d20-497f-8d36-045c701aea54 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0d650424-0d20-497f-8d36-045c701aea54');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ea55d8f3-f9d4-4710-8a94-080ddc18245c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ea55d8f3-f9d4-4710-8a94-080ddc18245c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ea55d8f3-f9d4-4710-8a94-080ddc18245c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 200,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 200,\n        \"samples\": [\n          \"Alice has 20 quarters. She wants to exchange them for nickels and so she goes to the bank. After getting back from the bank, she discovers that 20% of the nickels are iron nickels worth $3 each. What is the total value of her money now?\",\n          \"James creates a media empire.  He creates a movie for $2000.  Each DVD cost $6 to make.  He sells it for 2.5 times that much.  He sells 500 movies a day for 5 days a week.  How much profit does he make in 20 weeks?\",\n          \"Ann, Bill, Cate, and Dale each buy personal pan pizzas cut into 4 pieces. If Bill and Dale eat 50% of their pizzas and Ann and Cate eat 75% of the pizzas, how many pizza pieces are left uneaten?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 200,\n        \"samples\": [\n          \"A quarter is worth five nickels because .25 / .05 = <<.25/.05=5>>5\\nShe gets 100 nickels from the bank because 20 x 5 = <<20*5=100>>100\\n20 of the nickels are iron nickels because 100 x .20 = <<100*.20=20>>20\\n80 of the nickels are regular because 100 - 20 = <<100-20=80>>80\\nThe iron nickels are worth $60 because 20 x 3 = <<20*3=60>>60\\nThe regular nickels are worth $4 because 80 x .05 = <<80*.05=4>>4\\nHer money is now worth $64 because 60 + 4 = <<60+4=64>>64\\n#### 64\",\n          \"He sold each DVD for 6*2.5=$<<6*2.5=15>>15\\nSo he makes a profit of 15-6=$<<15-6=9>>9\\nSo each day he makes a profit of 9*500=$<<9*500=4500>>4500\\nSo he makes 4500*5=$<<4500*5=22500>>22,500\\nHe makes 22,500*20=$<<22500*20=450000>>450,000\\nThen after the cost of creating the movie he has a profit of 450,000-2000=$<<450000-2000=448000>>448,000\\n#### 448000\",\n          \"In total, there are 4 x 4 = <<4*4=16>>16 pizza pieces.\\nBill and Dale eat 2 x 4 x 50% = <<2*4*50*.01=4>>4 pieces.\\nAnn and Cate eat 2 x 4 x 75% = <<2*4*75*.01=6>>6 pieces.\\nThe four of them eat 4 + 6 = <<4+6=10>>10 pieces.\\nThere are 16 - 10 = <<16-10=6>>6 pizza pieces uneaten.\\n#### 6\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to parse the correct answer from the structured answer text\n",
        "def parse_correct_answer(entry):\n",
        "    answer_text = entry['answer']\n",
        "\n",
        "    # Use regex to find the final answer after '####'\n",
        "    match = re.search(r'####\\s*(\\d+)', answer_text)\n",
        "    if match:\n",
        "        # Convert answer to integer\n",
        "        return int(match.group(1))\n",
        "    return None\n",
        "\n"
      ],
      "metadata": {
        "id": "QhZT6JGQlMmi"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the OpenAI API to get a response\n",
        "def get_gpt3_response(prompt):\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. After solving the problem, put the final numerical answer preceded by \\n####\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=150,\n",
        "        temperature=0.3\n",
        "    )\n",
        "    # Extract and return the model's response\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n"
      ],
      "metadata": {
        "id": "gbTakDS4ngdx"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of function use\n",
        "question = df.iloc[0]['question']\n",
        "response = get_gpt3_response(question)\n",
        "print(\"Response:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "to7XjhbOniPQ",
        "outputId": "d085d607-d87c-406a-8360-da442477ff65"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: In April, Natalia sold 48 clips.\n",
            "In May, she sold half as many clips as in April, which is 48 / 2 = 24 clips.\n",
            "\n",
            "Altogether, Natalia sold 48 + 24 = 72 clips in April and May. \n",
            "\n",
            "#### 72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to assess answers and get model responses\n",
        "def assess_math_questions(df):\n",
        "    results = []\n",
        "    for index, row in df.iterrows():\n",
        "        question = row['question']\n",
        "        correct_answer = parse_correct_answer(row)\n",
        "        model_response = get_gpt3_response(question)\n",
        "        model_numerical_answer = int(re.findall(r'\\d+', model_response.strip())[-1]) if re.findall(r'\\d+', model_response.strip()) else None\n",
        "\n",
        "        # Store results in a dictionary for easier analysis later\n",
        "        results.append({\n",
        "            'question': question,\n",
        "            'correct_answer': correct_answer,\n",
        "            'model_response': model_response,\n",
        "            'model_numerical_answer': model_numerical_answer,\n",
        "            'is_correct': correct_answer == model_numerical_answer,\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n"
      ],
      "metadata": {
        "id": "rlVQ2nb-m8rG"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run assessment and view results\n",
        "results_df = assess_math_questions(df)\n",
        "results_df.head()\n",
        "print(results_df.head())\n",
        "\n",
        "# saving results as csv\n",
        "results_df.to_csv('math_results.csv', index=False)"
      ],
      "metadata": {
        "id": "1HqWIq31zPlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Accuracy of the model:', results_df['is_correct'].mean())\n"
      ],
      "metadata": {
        "id": "VSqQqjpo1qb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get device\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print('device is ', device)"
      ],
      "metadata": {
        "id": "q4EI_LpILkQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wq7WAWvelNeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GZVRmy5flNvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EgO-oXS0lN3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "plwZ72FVlN9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "login()\n",
        "\n"
      ],
      "metadata": {
        "id": "edkbnRcRKrpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using a Huggingface dataset\n",
        "\n",
        "\n",
        "\n",
        "ds = load_dataset(\"AI-MO/NuminaMath-CoT\")"
      ],
      "metadata": {
        "id": "Z7Ah9gwJLUtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Access the training split\n",
        "train_data = ds['train']\n"
      ],
      "metadata": {
        "id": "L2GyfuSzPOfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(train_data[0]['solution'])"
      ],
      "metadata": {
        "id": "P2cU6SuIgCc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dict = {}\n",
        "for split in ds.keys():\n",
        "  #print('splitting on ', split)\n",
        "  df_dict[split] = ds[split].to_pandas()\n",
        "#df_dict['train'].source.value_counts()\n",
        "train = df_dict['train'].copy()\n",
        "test = df_dict['test'].copy()\n",
        "\n",
        "test.head()"
      ],
      "metadata": {
        "id": "n7GfEyPPhh9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: find entries in the dataframe that need the escapechar\n",
        "\n",
        "rows_with_escapechar = []\n",
        "for index, row in test.iterrows():\n",
        "    if '\"' in row['problem'] or '\"' in row['solution']:\n",
        "        rows_with_escapechar.append(index)\n",
        "\n",
        "print(f\"Rows needing escapechar: {rows_with_escapechar}\")\n",
        "\n",
        "# Alternatively, to print the actual rows that need the escape character\n",
        "#for index in rows_with_escapechar:\n",
        "#  print(test.loc[index])"
      ],
      "metadata": {
        "id": "ZCz6-H9RhA2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Export train and test DataFrames to CSV files\n",
        "train.to_csv('math_train.csv', index=False, escapechar='/')\n",
        "test.to_csv('math_test.csv', index=False, escapechar='/')"
      ],
      "metadata": {
        "id": "F_BPLVAFfWdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# trying another ..."
      ],
      "metadata": {
        "id": "lxfPV5S9fW9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "file_path = 'CT_Protocol/data/results_df.csv'\n",
        "\n",
        "try:\n",
        "  results_df = pd.read_csv(file_path)\n",
        "  print(\"DataFrame created successfully.\")\n",
        "except FileNotFoundError:\n",
        "  print(f\"File not found at: {file_path}\")\n",
        "except Exception as e:\n",
        "  print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "dFeou_lr4low"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df.head()"
      ],
      "metadata": {
        "id": "GCaQd3aw__55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Extract the ground truth labels and predicted labels\n",
        "ground_truth = results_df['ft_protocol']\n",
        "predictions = results_df['ft_predicted_protocol']\n",
        "\n",
        "# Find the common protocols present in both ground_truth and predictions\n",
        "common_protocols = np.intersect1d(ground_truth.unique(), predictions.unique())\n",
        "\n",
        "# Filter the ground_truth and predictions to only include the common protocols\n",
        "filtered_ground_truth = ground_truth[ground_truth.isin(common_protocols)]\n",
        "filtered_predictions = predictions[ground_truth.isin(common_protocols)]\n",
        "\n",
        "# Create the confusion matrix for filtered labels\n",
        "cm = confusion_matrix(filtered_ground_truth, filtered_predictions, labels=common_protocols)\n",
        "\n",
        "# Plot the confusion matrix with labels for common protocols\n",
        "plt.figure(figsize=(10, 10))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=common_protocols, yticklabels=common_protocols)\n",
        "plt.xlabel('Predicted Protocol')\n",
        "plt.ylabel('Ground Truth Protocol')\n",
        "plt.title('Confusion Matrix for Fine-tuned Protocol Prediction')\n",
        "\n",
        "# Save the figure\n",
        "plt.savefig('/content/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wEwJ57bi4w9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "report = classification_report(filtered_ground_truth, filtered_predictions, labels=common_protocols)\n",
        "\n",
        "print(report)"
      ],
      "metadata": {
        "id": "iR53LYZK-dnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the ground truth labels and predicted labels\n",
        "results_bm_df = results_df[results_df['bm_predicted_protocol'].notna()]\n",
        "\n",
        "ground_truth = results_bm_df['bm_protocol']\n",
        "predictions = results_bm_df['bm_predicted_protocol']\n",
        "\n",
        "# Find the common protocols present in both ground_truth and predictions\n",
        "common_protocols = np.intersect1d(ground_truth.unique(), predictions.unique())\n",
        "\n",
        "# Filter the ground_truth and predictions to only include the common protocols\n",
        "filtered_ground_truth = ground_truth[ground_truth.isin(common_protocols)]\n",
        "filtered_predictions = predictions[ground_truth.isin(common_protocols)]\n",
        "\n",
        "# Create the confusion matrix for filtered labels\n",
        "cm = confusion_matrix(filtered_ground_truth, filtered_predictions, labels=common_protocols)\n",
        "\n",
        "# Plot the confusion matrix with labels for common protocols\n",
        "plt.figure(figsize=(10, 10))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=common_protocols, yticklabels=common_protocols)\n",
        "plt.xlabel('Predicted Protocol')\n",
        "plt.ylabel('Ground Truth Protocol')\n",
        "plt.title('Confusion Matrix for Fine-tuned Protocol Prediction')\n",
        "\n",
        "# Save the figure\n",
        "plt.savefig('/content/bm_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4o6WBcYJ_2G_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bm_report = classification_report(filtered_ground_truth, filtered_predictions, labels=common_protocols)\n",
        "\n",
        "print(bm_report)"
      ],
      "metadata": {
        "id": "hX157ATDBjFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filename = 'content/CT_Protocol/data/dataset031524.xlsx'\n",
        "# _, _, test_df = get_dataframes(filename)\n",
        "# model_path = \"mille055/auto_protocol2\"\n",
        "\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "# pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, framework=\"pt\")\n",
        "\n",
        "# ft_model_score = test_model2(test_df, pipe=pipe)\n",
        "# print(ft_model_score)"
      ],
      "metadata": {
        "id": "Of4tvTGOExh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# from google.colab import userdata\n",
        "# from huggingface_hub import HfApi\n",
        "# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "# import sys\n",
        "\n",
        "# sys.path.insert(0, 'content/ct_protocol/scripts')\n",
        "# from utilities import get_dataframes, test_model2\n",
        "\n",
        "\n",
        "# filename = 'content/CT_Protocol/data/dataset031524.xlsx'\n",
        "# _, _, test_df = get_dataframes(filename)\n",
        "# model_path = \"mille055/auto_protocol2\"\n",
        "\n",
        "# # Get the Hugging Face token from Colab secrets\n",
        "# try:\n",
        "#   token = userdata.get('huggingface_token')\n",
        "#   if token is None:\n",
        "#     raise ValueError(\"Hugging Face token not found in Colab secrets.\")\n",
        "# except:\n",
        "#     print(\"Error getting Hugging Face token from secrets. Please ensure it's set up.\")\n",
        "#     token = input(\"Enter your Hugging Face token: \")  # Prompt for manual input if secrets are not set up\n",
        "\n",
        "# api = HfApi(token=token)\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "# pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, framework=\"pt\")\n",
        "\n",
        "# ft_model_score = test_model2(test_df, pipe=pipe)\n",
        "# ft_model_score"
      ],
      "metadata": {
        "id": "LBVNAL5QFEb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4zfa35UEq_4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "config = PeftConfig.from_pretrained(\"mille055/auto_protocol2\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
        "model = PeftModel.from_pretrained(base_model, \"mille055/auto_protocol2\")"
      ],
      "metadata": {
        "id": "1cR-fOJVpOOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis\n",
        "\n",
        "Dataset:\n"
      ],
      "metadata": {
        "id": "FXJKPSMIttvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check multicollinearity using Variance Inflation Factor (VIF)\n",
        "def check_multicollinearity(df):\n",
        "  X = df.select_dtypes(include=[float, int])  # Select only numeric features\n",
        "\n",
        "  # Add constant for the intercept term\n",
        "  X = sm.add_constant(X)\n",
        "\n",
        "  # Calculate VIF\n",
        "  vif_data = pd.DataFrame()\n",
        "  vif_data['Feature'] = X.columns\n",
        "  vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "  print(vif_data)\n",
        "\n",
        "\n",
        "def EDA(df):\n",
        "  df1= df.copy()\n",
        "\n",
        "  # Display basic information about the dataset\n",
        "  dataset_info = {\n",
        "    'Shape': df1.shape,\n",
        "    'Columns': df1.columns.tolist(),\n",
        "    'Missing Values': df1.isnull().sum().sum(),\n",
        "    'Data Types': df1.dtypes.value_counts().to_dict(),\n",
        "    'Unique Values': df1.nunique().to_dict()\n",
        "  }\n",
        "\n",
        "  for key, value in dataset_info.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "\n",
        "  print(df1.head())\n",
        "\n",
        "  # Summary statistics\n",
        "  summary_statistics = df1.describe()\n",
        "  print(summary_statistics)\n",
        "\n",
        "  # Visualize numerical features\n",
        "  df1.hist(figsize=(12,10))\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  # Correlation matrix for numerical features after dropping any non-numeric columns\n",
        "\n",
        "  df1_numeric = df1.select_dtypes(include=['float64', 'int64'])\n",
        "  corr_matrix = df1_numeric.corr()\n",
        "  plt.figure(figsize=(10, 8))\n",
        "  sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\")\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  # # Pairplot - but takes awhile so only doing the first time\n",
        "  # print('\\nPairplot:\\n')\n",
        "  # sns.pairplot(df1)\n",
        "  # plt.show()\n",
        "\n",
        "  # Check colinearity\n",
        "  print('\\nColinearity Check:\\n')\n",
        "  check_multicollinearity(df1)\n",
        "\n",
        "\n",
        "EDA(df)"
      ],
      "metadata": {
        "id": "SGZIUIWKjVA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learned from the EDA:\n",
        "There are several points to take away from the EDA, including:\n",
        "\n",
        "1. There are 569 examples with no missing values.\n",
        "\n",
        "2. Average, median, and std dev valus of the features are given above.\n",
        "\n",
        "3. Features like mean radius (3817.26), mean perimeter (3792.70), worst radius (815.95), and worst perimeter (405.15) have extremely high VIFs, indicating that they are highly collinear with other features."
      ],
      "metadata": {
        "id": "YFiPUJ8RZKPd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare training and testing datasets"
      ],
      "metadata": {
        "id": "F_Q7AjcCMUuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparation of the train/test datasets\n",
        "\n",
        "X = df.drop(columns=['target'])\n",
        "y = df['target']\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "yXNj62eyK3jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rulefit"
      ],
      "metadata": {
        "id": "1_xVN5UxMdMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train RuleFit model\n",
        "rulefit_model = RuleFitClassifier(random_state=42)\n",
        "rulefit_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = rulefit_model.predict(X_test_scaled)\n",
        "print(f'Accuracy of RuleFit: {accuracy_score(y_test, y_pred):.2f}')"
      ],
      "metadata": {
        "id": "ScbqRThbMGqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rule_df = rulefit_model.visualize()\n",
        "rule_df"
      ],
      "metadata": {
        "id": "f1zjycIeMPBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repeating rulefit using a dataset with reduced number of features based on multicolinearity.\n",
        "\n"
      ],
      "metadata": {
        "id": "WvrHAOYAYENW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## removing some of the colinear features\n",
        "# List of features to remove based on prior analysis\n",
        "features_to_remove = [\n",
        "    'mean perimeter', 'mean area', 'perimeter error', 'area error',\n",
        "    'mean compactness', 'mean concave points', 'worst compactness', 'worst concave points',\n",
        "    'mean symmetry', 'symmetry error', 'worst symmetry',\n",
        "    'mean fractal dimension', 'worst fractal dimension', 'fractal dimension error'\n",
        "]\n",
        "\n",
        "# reducing to 16 features\n",
        "\n",
        "df_reduced = df.drop(columns=features_to_remove)\n",
        "print(df_reduced.shape)\n",
        "print(df_reduced.head())\n",
        "\n",
        "X_reduced = df_reduced.drop(columns=['target'])\n",
        "y = df_reduced['target']\n",
        "\n",
        "# splitting the dataset into train/test\n",
        "X_train_reduced, X_test_reduced, y_train, y_test = train_test_split(X_reduced, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_reduced_scaled = scaler.fit_transform(X_train_reduced)\n",
        "X_test_reduced_scaled = scaler.transform(X_test_reduced)\n"
      ],
      "metadata": {
        "id": "yClzXpKeX3t4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "E6JFDjdcYlLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train RuleFit model on the reduced dataset\n",
        "rulefit_reduced_model = RuleFitClassifier(random_state=42)\n",
        "rulefit_reduced_model.fit(X_train_reduced_scaled, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = rulefit_reduced_model.predict(X_test_reduced_scaled)\n",
        "print(f'Accuracy of RuleFit: {accuracy_score(y_test, y_pred):.2f}')"
      ],
      "metadata": {
        "id": "riMsScS_YTZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rule_reduced_df = rulefit_reduced_model.visualize()\n",
        "rule_reduced_df"
      ],
      "metadata": {
        "id": "MyJ78oLLZgK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A few points about rulefit for the reduced breast cancer dataset:\n",
        "\n",
        "1. The accuracy was high of 0.98 (compared with 0.96 for the original dataset containing all features).\n",
        "2. For the rules produced by the rulefit model:\n",
        "\n",
        "Individual Features (Linear Terms)\n",
        "\n",
        "*   X2 (mean texture) with a coefficient of -0.21: This means that higher values of mean texture are associated with a decrease in the target value (indicating benign characteristics if the target is malignancy).\n",
        "*   X11 (worst radius) with a coefficient of -0.20: Similarly, a larger worst radius slightly reduces the predicted outcome.\n",
        "\n",
        "Decision rules with multiple features\n",
        "* X12 <= 0.30526 and X3 <= 0.20041 and X4 <= 0.53781 has a coefficient of 1.50:\n",
        "This is the highest positive coefficient, meaning it's a significant predictor of a higher target outcome when all these conditions are true:\n",
        "worst perimeter ≤ 0.30526\n",
        "mean smoothness ≤ 0.20041\n",
        "mean concavity ≤ 0.53781\n",
        "This rule significantly influences the model's decision, suggesting it’s a combination suggestive of malignancy.\n",
        "\n",
        "\n",
        "* X10 <= 0.10695 and X12 <= 0.31888 and X14 <= 2.02129 and X4 <= 2.11819 has a positive coefficient of 0.78, indicating it has a strong effect when these conditions hold true:\n",
        "X10 (concave points error) <= 0.10695\n",
        "X12 (worst perimeter) <= 0.31888\n",
        "X14 (worst smoothness) <= 2.02129\n",
        "X4 (mean concavity) <= 2.11819\n",
        "\n",
        "* X11 > -0.93104 and X12 > -0.16385 and X15 > -0.35809 has a coefficient of -1.18, which is a high negative coefficient. When these conditions are met:\n",
        "worst radius is higher than threshold,\n",
        "worst perimeter is higher than threhsold,\n",
        "worst concavity is greater than threshold. These findings suggest benign characteristics.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mxDGcXsta4bb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizations of rulefit"
      ],
      "metadata": {
        "id": "lP4b8jVYfwRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Given rules and coefficients from your RuleFit model\n",
        "rules_data = {\n",
        "    'X3 <= 0.27437 and X4 <= 0.76102': 0.09,\n",
        "    'X10 <= 0.10695 and X12 <= 0.31888 and X14 <= 2.02129 and X4 <= 2.11819': 0.78,\n",
        "    'X12 <= 0.30526 and X3 <= 0.20041 and X4 <= 0.53781': 1.50,\n",
        "    'X1 <= 0.48015 and X12 <= 0.21144': 0.27,\n",
        "    'X11 > -0.93104 and X12 > -0.16385 and X15 > -0.35809': -1.18,\n",
        "    'X2': -0.21,\n",
        "    'X4': -0.00,\n",
        "    'X7': 0.03,\n",
        "    'X11': -0.20\n",
        "}\n",
        "\n",
        "# Map the feature indices to their actual names\n",
        "feature_mapping = {\n",
        "    'X1': 'mean radius',\n",
        "    'X2': 'mean texture',\n",
        "    'X3': 'mean smoothness',\n",
        "    'X4': 'mean concavity',\n",
        "    'X7': 'smoothness error',\n",
        "    'X10': 'concave points error',\n",
        "    'X11': 'worst radius',\n",
        "    'X12': 'worst texture',\n",
        "    'X14': 'worst smoothness',\n",
        "    'X15': 'worst concavity'\n",
        "}\n",
        "\n",
        "# rules and feature names to be more interpretable\n",
        "\n",
        "interpreted_rules = {}\n",
        "for rule, coef in rules_data.items():\n",
        "    interpreted_rule = rule\n",
        "    for old, new in feature_mapping.items():\n",
        "        interpreted_rule = interpreted_rule.replace(old, new)\n",
        "    interpreted_rules[interpreted_rule] = coef\n",
        "\n",
        "# Separate out the linear terms and rules\n",
        "linear_terms = {k: v for k, v in interpreted_rules.items() if not ('and' in k)}\n",
        "rules_only = {k: v for k, v in interpreted_rules.items() if 'and' in k}\n",
        "\n",
        "# Plot linear terms\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(list(linear_terms.keys()), list(linear_terms.values()), color='skyblue')\n",
        "plt.title('Linear Terms Contribution (Specific to Dataset)')\n",
        "plt.xlabel('Coefficient Value')\n",
        "plt.ylabel('Features')\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot rules contribution\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh(list(rules_only.keys()), list(rules_only.values()), color='lightcoral')\n",
        "plt.title('Decision Rules Contribution (Specific to Dataset)')\n",
        "plt.xlabel('Coefficient Value')\n",
        "plt.ylabel('Rules')\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NogbQJYmZyhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rulefit_image_path = os.path.join('AIPI590-XAI', 'Assignments', 'Visualizations', 'rulefit.png')\n",
        "Image(filename=rulefit_image_path)\n"
      ],
      "metadata": {
        "id": "wny3voTLfJk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Boosted Rules Classifier (BRC)\n",
        "\n",
        "The BoostedRulesClassifier is an interpretable machine learning model that combines the power of rule-based models with boosting techniques to create a model that is both accurate and understandable. It is in the family of ensemble models, specifically designed to handle complex datasets while maintaining transparency in its decision-making process.\n",
        "\n",
        "The model starts by generating a set of simple decision rules from the features of the dataset. These rules are like \"if-then\" statements, e.g., \"if mean radius > 15 and mean texture < 20, then predict malignant.\"\n",
        "Each rule is created using decision tree splits. The boosting is an iterative process where the model learns from its mistakes. Initially, the model creates a simple rule and makes predictions, and refines this based on the early predictions."
      ],
      "metadata": {
        "id": "DkTqVXannwiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "brc_model = BoostedRulesClassifier(random_state=42)\n",
        "brc_model.fit(X_train_reduced_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = brc_model.predict(X_test_reduced_scaled)\n",
        "\n",
        "# Assess model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy of BoostedRulesClassifier: {accuracy:.2f}')\n",
        "\n",
        "# Detailed classification report\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "id": "Xvftcnc2pVPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the rules from the BoostedRulesClassifier\n",
        "# Access the individual base estimators (decision trees) from the BoostedRulesClassifier\n",
        "base_estimators = brc_model.estimators_\n",
        "\n",
        "# Check how many base estimators (rules) are present\n",
        "print(f\"Number of base estimators (trees): {len(base_estimators)}\")\n",
        "\n",
        "# Extract text\n",
        "for i, estimator in enumerate(base_estimators[:5]):  # Extract rules from the first 5 trees\n",
        "    print(f\"Rules from tree {i + 1}:\\n\")\n",
        "    print(export_text(estimator, feature_names=X_reduced.columns.tolist()))\n",
        "    print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
        "\n",
        "\n",
        "# Access feature importances from the BoostedRulesClassifier\n",
        "feature_importances = brc_model.feature_importances_\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': X_reduced.columns,\n",
        "    'Importance': feature_importances\n",
        "})\n",
        "\n",
        "# Sort features by importance\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plot the feature importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='lightcoral')\n",
        "plt.title('Feature Importance from BoostedRulesClassifier')\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fY-PoM5NoFFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## visualize schematic of Boosted Rules Classifier\n",
        "\n",
        "brc_image_path = os.path.join('AIPI590-XAI', 'Assignments', 'Visualizations', 'brc.png')\n",
        "Image(filename=brc_image_path)"
      ],
      "metadata": {
        "id": "nAYQ82kOssW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SLIM (Supersparse Linear Integer Model)\n",
        "SLIM is an interpretable machine learning model that aims to create a linear model with integer coefficients. The model generates sparse models (many coefficients are zero), meaning it selects only a few important features, and uses integer weights (e.g., -2, 1, 3), making it very easy to understand. The model balances accuracy and interpretability by being both sparse and using simple, integer-based coefficients. While SLIM can be computationally intensive for larger datasets or datasets with many features because it searches for the optimal sparse model, this isn't a significant issue for the Breast Cancer dataset.\n",
        "\n",
        "As stated in the paper [1], the model comprises a scoring system which, \"are linear classification models that only require users to add, subtract and multiply a few small numbers in order to make a prediction. These models are used to assess the risk of numerous serious medical conditions since they allow physicians to make quick predictions, without extensive training, and without the use of a computer.\" [1] https://arxiv.org/pdf/1502.04269"
      ],
      "metadata": {
        "id": "4EjqxPBM0Csx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and fit the SLIM model\n",
        "slim_model = SLIMClassifier()\n",
        "slim_model.fit(X_train_reduced_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = slim_model.predict(X_test_reduced_scaled)"
      ],
      "metadata": {
        "id": "dQkpoFikyyAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy of SLIM: {accuracy:.2f}')\n",
        "\n",
        "# Detailed classification report\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Access the internal model from slim_model\n",
        "internal_model = slim_model.model_\n",
        "\n",
        "# Extract the coefficients\n",
        "coefficients = internal_model.coef_\n",
        "print(\"SLIM Model Coefficients:\", coefficients)"
      ],
      "metadata": {
        "id": "GGlrA-uH0o_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the internal model from slim_model\n",
        "internal_model = slim_model.model_\n",
        "\n",
        "# Extract the coefficients\n",
        "coefficients = internal_model.coef_\n",
        "print(\"SLIM Model Coefficients:\", coefficients)"
      ],
      "metadata": {
        "id": "UM_YV9RP0t0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coef_list = zip(X_reduced.columns, coefficients[0])\n",
        "coef_df = pd.DataFrame(coef_list, columns=['Feature', 'Coefficient'])\n",
        "coef_df"
      ],
      "metadata": {
        "id": "t-tibtu81vi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rulefit_image_path = os.path.join('AIPI590-XAI', 'Assignments', 'Visualizations', 'slim.png')\n",
        "Image(filename=rulefit_image_path)"
      ],
      "metadata": {
        "id": "ppo0ipf8_dtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The models have similar accuracy in predicting malignancy (98%). The relative importance of certain features differs (in terms of feature-importance or their coefficients in a linear model) are somewhat different among the three models.\n",
        "\n",
        "With regards to Interpretability:\n",
        "* **SLIM Model:**\n",
        "The SLIM model provides high interpretability because it uses sparse linear equations with integer coefficients. Each feature's contribution is easily understandable, making it clear how predictions are made. The integer coefficients indicate the weight or impact of each feature on the prediction, which is ideal for explaining model behavior to non-experts. The model's sparsity means that only a few features are selected, making it easier to identify the most important factors influencing the prediction but possibly limiting accuracy (although not in the case of this dataset).\n",
        "\n",
        "* **Boosted Rules Classifier and RuleFit:**\n",
        "The Boosted Rules Classifier is also interpretable but combines multiple rules in an ensemble format, which can make it slightly harder to interpret compared to SLIM’s linear equation. It can possibly capture more complex relaitonships.\n",
        "\n",
        "  RuleFit provides linear terms and decision rules, offering a good balance between accuracy and interpretability, but it is more complex than SLIM’s integer-based linear model. RuleFit captures feature importance through both linear terms and rules but may include more features, making it less sparse compared to SLIM."
      ],
      "metadata": {
        "id": "v3Bk0WuZ3ccb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9mYl4G145bo0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}