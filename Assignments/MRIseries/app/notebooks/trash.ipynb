{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLAB_FLAG = False   # whether running on colab or locally on computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COLAB_FLAG:\n",
    "    !pip install pydicom==2.1.2\n",
    "    !pip install monai seaborn sentence_transformers\n",
    "    !git clone 'https://github.com/mille055/AIPI540_individual_project.git'\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import pydicom\n",
    "#import monai\n",
    "import pickle\n",
    "import glob\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, recall_score, plot_confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from pprint import pprint\n",
    "from fastai.basics import delegates\n",
    "from fastcore.parallel import parallel\n",
    "from fastcore.utils import gt\n",
    "from fastcore.foundation import L\n",
    "\n",
    "# import monai\n",
    "# from monai.data import DataLoader, ImageDataset\n",
    "# from monai.transforms import AddChannel, Compose, RandRotate90, Resize, ScaleIntensity, EnsureType\n",
    "from pydicom.dataset import Dataset as DcmDataset\n",
    "from pydicom.tag import BaseTag as DcmTag\n",
    "from pydicom.multival import MultiValue as DcmMultiValue\n",
    "import sys\n",
    "import importlib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#local imports\n",
    "if COLAB_FLAG:\n",
    "    sys.path.append('/content/AIPI540_individual_project/scripts/')\n",
    "    train_datafile = '/content/AIPI540_individual_project/data/trainfiles.csv'\n",
    "    val_datafile = '/content/AIPI540_individual_project/data/valfiles.csv'\n",
    "    test_datafile = '/content/AIPI540_individual_project/data/testfiles.csv'\n",
    "\n",
    "else: # running locally\n",
    "    sys.path.append('/Users/cmm/Documents/GitHub/AIPI540_individual_project/scripts/')\n",
    "    train_datafile = '../data/trainfiles.csv'\n",
    "    val_datafile = '../data/valfiles.csv'\n",
    "    test_datafile = '../data/testfiles.csv'\n",
    "\n",
    "### local imports ###\n",
    "from config import file_dict, feats, feats_to_keep, column_lists, RF_parameters, classes, model_paths\n",
    "from config import abd_label_dict, val_list, train_val_split_percent, random_seed, data_transforms\n",
    "from config import sentence_encoder, series_description_column\n",
    "from utils import *\n",
    "#from train_pixel_model import train_pix_model, test_pix_model, ImgDataset, image_to_tensor, get_pixel_preds_and_probs\n",
    "#from train_meta_model import train_fit_parameter_trial, train_meta_model, calc_feature_importances, get_meta_probs, meta_inference\n",
    "#from train_text_model import train_text_log_model, load_text_data, get_nlp_inference\n",
    "from NLP.NLP_inference import get_NLP_inference\n",
    "from NLP.NLP_training import train_NLP_model\n",
    "from cnn.cnn_dataset import ImgDataset\n",
    "from cnn.cnn_inference import image_to_tensor, pixel_inference, test_pix_model, load_pixel_model, visualize_results\n",
    "from cnn.cnn_model import CustomResNet50\n",
    "from cnn.cnn_data_loaders import get_data_loaders\n",
    "from metadata.meta_inference import meta_inference, calc_feature_importances\n",
    "from metadata.meta_training import train_fit_parameter_trial, train_meta_model, evaluate_meta_model\n",
    "from fusion_model.fus_model import FusionModel\n",
    "from fusion_model.fus_inference import get_fusion_inference\n",
    "from model_container import ModelContainer\n",
    "from process_tree import Processor, write_labels_into_dicom\n",
    "# from AIPI540_individual_project.scripts.train_pixel_model import train_model\n",
    "# from AIPI540_individual_project.scripts.train_text_model import load_text_data, train_text_model, list_incorrect_text_predictions\n",
    "# from AIPI540_individual_project.scripts.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_model = FusionModel\n",
    "\n",
    "class PartialFusionModel(nn.Module):\n",
    "    def __init__(self, model1, model2, num_classes):\n",
    "        super(PartialFusionModel, self).__init__()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        #self.model3 = model3\n",
    "        self.fusion_layer = nn.Linear(num_classes * 2, num_classes)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        \n",
    "        x = torch.cat((x1, x2), dim=0)\n",
    "        x = self.fusion_layer(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "partial_fusion_model = PartialFusionModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FusionModel(\n",
      "  (model2): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Linear(in_features=2048, out_features=19, bias=True)\n",
      "  )\n",
      "  (fusion_layer): Linear(in_features=57, out_features=19, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "with open('../models/fusion_model041623.pkl', 'rb') as file:\n",
    "    fusion = pickle.load(file)\n",
    "\n",
    "print(fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### fix my fusion model issue:; have to change from a pickled model to saved weights\n",
    "def save_fusion_model_weights(pickle_path, weight_path):\n",
    "    with open(pickle_path, 'rb') as file:\n",
    "        fusion_model = pickle.load(file)\n",
    "    torch.save(fusion_model.state_dict(), weight_path)\n",
    "\n",
    "save_fusion_model_weights(model_paths['fusion'], '../models/fusion_saved_weights042123.pth')\n",
    "save_fusion_model_weights('../models/meta_and_pixel_fusion_model041623.pkl', '../models/fusion_saved_weights_no_nlp042123.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### in the fusion model creation code:\n",
    "\n",
    "Xtrain1 = fusion_train_df.meta_probs.values\n",
    "\n",
    "Xtrain2 = fusion_train_df.pixel_probs.values\n",
    "Xtrain3 = fusion_train_df.nlp_probs.values\n",
    "\n",
    "Xtrain1 = np.stack(Xtrain1, axis=0)\n",
    "Xtrain2 = np.stack(Xtrain2, axis=0)\n",
    "Xtrain3 = np.stack(Xtrain3, axis=0)\n",
    "\n",
    "out_features = 19\n",
    "\n",
    "\n",
    "y_train_fusion = np.array([classes.index(x) for x in fusion_train_df.true])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickling the dataframes which have undergone inference from the base models\n",
    "\n",
    "# fusion_train_df.to_pickle('../data/fusion_train.pkl')\n",
    "# fusion_val_df.to_pickle('../data/fusion_val.pkl')\n",
    "# fusion_test_df.to_pickle('../data/fusion_test.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FusionModel(nn.Module):\n",
    "#     def __init__(self, model1, model2, model3, num_classes):\n",
    "#         super(FusionModel, self).__init__()\n",
    "#         self.model1 = model1\n",
    "#         self.model2 = model2\n",
    "#         self.model3 = model3\n",
    "#         self.fusion_layer = nn.Linear(num_classes * 3, num_classes)\n",
    "\n",
    "#     def forward(self, x1, x2, x3):\n",
    "        \n",
    "#         x = torch.cat((x1, x2, x3), dim=0)\n",
    "#         x = self.fusion_layer(x)\n",
    "\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartialFusionModel(nn.Module):\n",
    "    def __init__(self, model1, model2, num_classes):\n",
    "        super(PartialFusionModel, self).__init__()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        #self.model3 = model3\n",
    "        self.fusion_layer = nn.Linear(num_classes * 2, num_classes)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        \n",
    "        x = torch.cat((x1, x2), dim=0)\n",
    "        x = self.fusion_layer(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "# ## since I merged this into FusionModel with conditions after saving the weights, i need to try to recover the weights into \n",
    "# ## the new mdoel\n",
    "# # load components\n",
    "\n",
    "\n",
    "# #pmodel = PartialFusionModel(model_pix, meta_model, num_classes=19)\n",
    "\n",
    "# from config import model_paths\n",
    "# model_container = ModelContainer()\n",
    "\n",
    "# with open(model_paths['fusion_no_nlp'], 'rb') as file:\n",
    "#     partial_fusion_model = pickle.load(file)\n",
    "\n",
    "# fusion_model_with_partial_weights = FusionModel(models, num_classes=19, include_nlp=False)\n",
    "# fusion_model_with_partial_weights.fusion_layer.weight = nn.Parameter(partial_fusion_model.fusion_layer.weight)\n",
    "\n",
    "# # Save only the model weights (state_dict)\n",
    "# torch.save(fusion_model_with_partial_weights.state_dict(), '../models/fusion_model_no_nlp042123.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fusion_model = FusionModel\n",
    "\n",
    "\n",
    "# num_classes = 19  # Number of classes\n",
    "# pixel_model = model\n",
    "# fusion_model = FusionModel(meta_model, pixel_model, NLP_model, num_classes)\n",
    "# meta_and_pixel_fusion_model = PartialFusionModel(meta_model, pixel_model, num_classes)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(fusion_model.parameters(), lr=0.001)\n",
    "# p_optimizer = torch.optim.Adam(meta_and_pixel_fusion_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 20\n",
    "# batch_size = 32\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     for i in range(0, len(y_train_fusion), batch_size):\n",
    "#         actual_batch_size = min(batch_size, len(Xtrain1) - i)\n",
    "#         X1_batch = torch.tensor(np.array(Xtrain1[i:i+actual_batch_size], dtype=np.float32), dtype=torch.float32)\n",
    "#         X2_batch = torch.tensor(np.array(Xtrain2[i:i+actual_batch_size], dtype=np.float32), dtype=torch.float32)\n",
    "#         X3_batch = torch.tensor(np.array(Xtrain3[i:i+actual_batch_size], dtype=np.float32), dtype=torch.float32)\n",
    "#         y_batch = torch.tensor(y_train_fusion[i:i+actual_batch_size], dtype=torch.long)\n",
    "\n",
    "#         # print(f\"X1_batch shape: {X1_batch.shape}\")\n",
    "#         # print(f\"X2_batch shape: {X2_batch.shape}\")\n",
    "#         # print(f\"X3_batch shape: {X3_batch.shape}\")\n",
    "#         # print(f\"y_batch shape: {y_batch.shape}\")\n",
    "\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = fusion_model(X1_batch, X2_batch, X3_batch)\n",
    "#         loss = criterion(outputs, y_batch)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xtest1 = fusion_test_df.meta_probs.values\n",
    "# Xtest2 = fusion_test_df.pixel_probs.values\n",
    "# Xtest3 = fusion_test_df.nlp_probs.values\n",
    "\n",
    "# Xtest1 = np.stack(Xtest1, axis=0)\n",
    "# Xtest2 = np.stack(Xtest2, axis=0)\n",
    "# Xtest3 = np.stack(Xtest3, axis=0)\n",
    "\n",
    "# out_features = 19\n",
    "\n",
    "# actual_classes = [0,2,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,23,25]\n",
    "# y_test_fusion = np.array([actual_classes.index(x) for x in fusion_test_df.true])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn.functional as F\n",
    "# # Evaluate the model with the test data\n",
    "# with torch.no_grad():\n",
    "#     X1_test_t = torch.tensor(Xtest1, dtype=torch.float32)\n",
    "#     X2_test_t = torch.tensor(Xtest2, dtype=torch.float32)\n",
    "#     X3_test_t = torch.tensor(Xtest3, dtype=torch.float32)\n",
    "#     y_test_t = torch.tensor(y_test_fusion, dtype=torch.long)\n",
    "\n",
    "#     outputs = fusion_model(X1_test_t, X2_test_t, X3_test_t)\n",
    "#     f_probabilities = F.softmax(outputs, dim=1)  # Apply softmax to the outputs\n",
    "\n",
    "#     _, f_predicted = torch.max(outputs, 1)\n",
    "#     correct = (f_predicted == y_test_t).sum().item()\n",
    "#     f_accuracy = correct / len(y_test_t) * 100\n",
    "\n",
    "# print(f'Test accuracy: {f_accuracy}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_batch(df, data_dir, destination_folder, write_labels=True):\n",
    "\n",
    "#     df1 = df.copy()\n",
    "#     #print('In batch, columns are: ', df1.columns)\n",
    "#     batch = df1.groupby('patientID').apply(lambda x: process_patient(x, data_dir, destination_folder, write_labels))\n",
    "    \n",
    "#     # print('writing labels into dicom in location ', dest_name)\n",
    "#     # for filename in batch.fname:\n",
    "\n",
    "#     return batch\n",
    "\n",
    "# def process_patient(patient_df, data_dir, destination_folder, write_labels):\n",
    "#     processed_exams = patient_df.groupby('exam').apply(lambda x: process_exam(x, data_dir, destination_folder, write_labels))\n",
    "#     return processed_exams\n",
    "\n",
    "\n",
    "# def process_exam(exam_df, data_dir, destination_folder, write_labels):\n",
    "#     # Group exam data by series and apply the process_series function\n",
    "#     processed_series = exam_df.groupby('series').apply(lambda x: process_series(x, data_dir, destination_folder, write_labels))\n",
    "\n",
    "#     #result = pd.concat(processed_series)\n",
    "#     result = processed_series\n",
    "#     return result\n",
    "    \n",
    "# def process_series(series_df, data_dir, destination_folder, write_labels, selection_fraction=0.5):\n",
    "#     # Sort the dataframe by file_info (or another relevant column)\n",
    "#     sorted_series = series_df.sort_values(by='fname')\n",
    "\n",
    "#     # Find the middle image index\n",
    "#     middle_index = int(len(sorted_series) * selection_fraction)\n",
    "\n",
    "#     # Get the middle image\n",
    "#     middle_image = sorted_series.iloc[middle_index]\n",
    "\n",
    "#     predicted_series_class, predicted_series_confidence = get_fusion_inference(middle_image)\n",
    "\n",
    "#     sorted_series['predicted_class'] = predicted_series_class\n",
    "#     sorted_series['prediction_confidence'] = np.round(predicted_series_confidence, 2)\n",
    "\n",
    "#     #save_path = f'/volumes/cm7/processed/modified/{series_df.patientID}/{series_df.exam}/{series_df[\"series\"]}/'\n",
    "\n",
    "#      # Define the save path relative to data_dir\n",
    "#     relative_path = os.path.relpath(series_df.fname.iloc[0], data_dir)\n",
    "#     save_path = os.path.join(data_dir, destination_folder, os.path.dirname(relative_path))\n",
    "\n",
    "#     if not os.path.exists(save_path):\n",
    "#         os.makedirs(save_path)\n",
    "\n",
    "#     if write_labels:\n",
    "#         #print('writing new data into', save_path)\n",
    "#         write_labels_into_dicom(sorted_series, label_num=predicted_series_class,\n",
    "#                             conf_num=np.round(predicted_series_confidence, 3), path=save_path)\n",
    "\n",
    "#     return sorted_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(df, data_dir, destination_folder, write_labels=True):\n",
    "\n",
    "    df1 = df.copy()\n",
    "    #print('In batch, columns are: ', df1.columns)\n",
    "    batch = df1.groupby('patientID').apply(lambda x: process_patient(x, data_dir, destination_folder, write_labels))\n",
    "    \n",
    "    # print('writing labels into dicom in location ', dest_name)\n",
    "    # for filename in batch.fname:\n",
    "\n",
    "    return batch\n",
    "\n",
    "def process_patient(patient_df, data_dir, destination_folder, write_labels):\n",
    "    processed_exams = patient_df.groupby('exam').apply(lambda x: process_exam(x, data_dir, destination_folder, write_labels))\n",
    "    return processed_exams\n",
    "\n",
    "\n",
    "def process_exam(exam_df, data_dir, destination_folder, write_labels):\n",
    "    # Group exam data by series and apply the process_series function\n",
    "    processed_series = exam_df.groupby('series').apply(lambda x: process_series(x, data_dir, destination_folder))\n",
    "\n",
    "    #result = pd.concat(processed_series)\n",
    "    result = processed_series\n",
    "    return result\n",
    "    \n",
    "def process_series(series_df, data_dir, destination_folder, write_labels, selection_fraction=0.5):\n",
    "    # Sort the dataframe by file_info (or another relevant column)\n",
    "    sorted_series = series_df.sort_values(by='fname')\n",
    "\n",
    "    # Find the middle image index\n",
    "    middle_index = int(len(sorted_series) * selection_fraction)\n",
    "\n",
    "    # Get the middle image\n",
    "    middle_image = sorted_series.iloc[middle_index]\n",
    "\n",
    "    predicted_series_class, predicted_series_confidence = get_fusion_model_prediction(middle_image)\n",
    "\n",
    "    sorted_series['predicted_class'] = predicted_series_class\n",
    "    sorted_series['prediction_confidence'] = np.round(predicted_series_confidence, 2)\n",
    "\n",
    "    #save_path = f'/volumes/cm7/processed/modified/{series_df.patientID}/{series_df.exam}/{series_df[\"series\"]}/'\n",
    "\n",
    "     # Define the save path relative to data_dir\n",
    "    relative_path = os.path.relpath(series_df.fname.iloc[0], data_dir)\n",
    "    save_path = os.path.join(data_dir, destination_folder, os.path.dirname(relative_path))\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    if write_labels:\n",
    "        #print('writing new data into', save_path)\n",
    "        write_labels_into_dicom(sorted_series, label_num=predicted_series_class,\n",
    "                            conf_num=np.round(predicted_series_confidence, 3), path=save_path)\n",
    "\n",
    "    return sorted_series\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_new_image_df(data_dir, dest_name='modified', write_labels = True):\n",
    "    # create the df of image data    \n",
    "    _, df = get_dicoms(data_dir)\n",
    "    df1 = df.copy()\n",
    "    ## manipulate df prior to evaluation\n",
    "    df1 = expand_filename(df1, ['blank', 'filename', 'series', 'exam', 'patientID'])\n",
    "    df1.drop(columns='blank', inplace=True)\n",
    "    df1['file_info']=df1.fname\n",
    "    df1['img_num'] = df1.file_info.apply(extract_image_number)\n",
    "    df1['contrast'] = df1.apply(detect_contrast, axis=1)\n",
    "    df1['plane'] = df1.apply(compute_plane, axis=1)\n",
    "    df1['series_num'] = df1.series.apply(lambda x: str(x).split('_')[-1])\n",
    "    #print('columns before preprocess are', df1.columns)\n",
    "\n",
    "    df1 = preprocess(df1)\n",
    "   # print('after preprocessin exam is in columns?', ('exam' in df1.columns))\n",
    "    #print('after preprocessing series is in columns?', ('series' in df1.columns))\n",
    "    #process the batch of studies\n",
    "    processed_frame = process_batch(df1, data_dir, dest_name, write_labels)\n",
    "\n",
    "    return processed_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_one_image(series, selection_fraction=0.5):\n",
    "    sorted_series = series.sort_values()\n",
    "    #print(sorted_series)\n",
    "    index = int(len(sorted_series) * selection_fraction)\n",
    "    print(index, len(sorted_series))\n",
    "    return sorted_series.iloc[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_one_from_series2(df, selection_fraction=0.5):\n",
    "    def select_one_image(series):\n",
    "        sorted_series = series.sort_values()\n",
    "        index = int(len(sorted_series) * selection_fraction)\n",
    "        return sorted_series.iloc[index]\n",
    "\n",
    "    df = df.sort_values(by=['patientID', 'series', 'file_info'])\n",
    "\n",
    "    grouped_df = df.groupby(['patientID', 'series'])\n",
    "    selected_rows = grouped_df.agg(select_one_image).reset_index()\n",
    "    \n",
    "    return selected_rows\n",
    "\n",
    "# Example usage\n",
    "# data = {'patientID': [1, 1, 1, 2, 2, 3, 3, 3, 3],\n",
    "#         'series': [1, 1, 2, 1, 2, 1, 1, 2, 2],\n",
    "#         'file_info': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I']}\n",
    "\n",
    "# df = pd.DataFrame(data)\n",
    "# selected_rows = mask_one_from_series(df, selection_fraction=0.5)\n",
    "# print(selected_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def filter_out_mv(df):\n",
    "#     # Create a boolean mask to identify rows with MultiValue objects in any column\n",
    "#     mask = df.applymap(lambda x: isinstance(x, MultiValue)).any(axis=1)\n",
    "#     #print(df[mask])\n",
    "#     # Invert the mask to select rows without MultiValue objects\n",
    "#     rows_without_multivalue = ~mask\n",
    "\n",
    "#     # Filter out rows with MultiValue objects in any column\n",
    "#     filtered_df = df[rows_without_multivalue]\n",
    "\n",
    "#     return mask, filtered_df\n",
    "\n",
    "#from pydicom.multival import MultiValue\n",
    "# def compute_plane_new(row):\n",
    "#     '''\n",
    "#     Computes the plane of imaging from the direction cosines provided in the `ImageOrientationPatient` field.\n",
    "#     The format of the values in this field is: `[x1, y1, z1, x2, y2, z2]`,\n",
    "#     which correspond to the direction cosines for the first row and column of the image pixel data.\n",
    "#     '''\n",
    "#     planes = ['sag', 'cor', 'ax']\n",
    "#     if 'ImageOrientationPatient1' in row.keys():\n",
    "#         dircos = [v for k, v in row.items() if 'ImageOrientationPatient' in k]\n",
    "#     else:\n",
    "#         dircos = row['ImageOrientationPatient']\n",
    "\n",
    "#         # Handle MultiValue objects by converting them to a list of floats\n",
    "#         if isinstance(dircos, MultiValue):\n",
    "#             dircos = [float(x) for x in dircos]\n",
    "\n",
    "#     # Check if dircos has the expected length\n",
    "#     if not isinstance(dircos, float) and len(dircos) == 6:\n",
    "#         dircos = np.array(dircos).reshape(2, 3)\n",
    "#         pnorm = abs(np.cross(dircos[0], dircos[1]))\n",
    "#         return planes[np.argmax(pnorm)]\n",
    "#     else:\n",
    "#         return 'unknown'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  # for idx, image_file in enumerate(selected_images):\n",
    "            #     with cols[idx % 4]:\n",
    "            #         dcm_data = pydicom.dcmread(image_file)\n",
    "            #         image = Image.fromarray(dcm_data.pixel_array)\n",
    "            #         image = image.convert(\"L\")  # Convert the image to grayscale mode\n",
    "            #         st.image(image, caption=os.path.basename(image_file), use_column_width=True)\n",
    "\n",
    "# def apply_window_level(image, window_center, window_width):\n",
    "#     min_value = window_center - window_width // 2\n",
    "#     max_value = window_center + window_width // 2\n",
    "#     image = np.clip(image, min_value, max_value)\n",
    "\n",
    "#     def normalize_array(arr):\n",
    "#         arr_min, arr_max = arr.min(), arr.max()\n",
    "#         return (arr - arr_min) * 255 / (arr_max - arr_min)\n",
    "\n",
    "#     return normalize_array(image)\n",
    "\n",
    "# start_folder = '/volumes/cm7/start_folder'\n",
    "# dicom_files = load_dicom_images(start_folder)\n",
    "\n",
    "# patient_list = sorted(list(set([os.path.basename(os.path.dirname(f)) for f in dicom_files])))\n",
    "# selected_patient = st.sidebar.selectbox(\"Select a Patient\", patient_list)\n",
    "\n",
    "# exam_list = sorted(list(set([os.path.basename(os.path.dirname(os.path.dirname(f))) for f in dicom_files if selected_patient in f])))\n",
    "# selected_exam = st.sidebar.selectbox(\"Select an Exam\", exam_list)\n",
    "\n",
    "# series_list = sorted(list(set([os.path.basename(f) for f in dicom_files if selected_exam in f])))\n",
    "# selected_series = st.selectbox(\"Select a Series\", series_list)\n",
    "\n",
    "# selected_images = [f for f in dicom_files if selected_series in f]\n",
    "# selected_images.sort(key=lambda x: os.path.basename(x))  # Sort images within each series by filename\n",
    "\n",
    "# # Process images button\n",
    "# process_images = st.button(\"Process Images\")\n",
    "\n",
    "# if process_images:\n",
    "#     # Classify the series with your machine learning model\n",
    "#     #predicted_type = classify_series(selected_images)  # Replace with the appropriate function call\n",
    "#     st.write(f\"Predicted Type: not implemented yet\")\n",
    "\n",
    "# image_idx = st.slider(\"Select Image Index\", 0, len(selected_images) - 1, 0)\n",
    "\n",
    "# window_center = st.slider(\"Window Center\", 0, 4096, 2048)\n",
    "# window_width = st.slider(\"Window Width\", 0, 4096, 4096)\n",
    "\n",
    "# with st.container():\n",
    "#     image_file = selected_images[image_idx]\n",
    "#     try:\n",
    "#         dcm_data = pydicom.dcmread(image_file)\n",
    "#         image = dcm_data.pixel_array\n",
    "#         image = apply_window_level(image, window_center, window_width)\n",
    "#         image = Image.fromarray(np.uint8(image))\n",
    "#         st.image(image, caption=os.path.basename(image_file), use_column_width=True)\n",
    "#     except Exception as e:\n",
    "#         pass  # Ignore the file and do nothing\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # # Function to load DICOM files into a dataframe\n",
    "# # def load_dicom_data(folder):\n",
    "# #     data = []\n",
    "# #     for root, _, files in os.walk(folder):\n",
    "# #         for file in files:\n",
    "# #             if file.lower().endswith(\".dcm\"):\n",
    "# #                 try:\n",
    "# #                     dcm_file_path = os.path.join(root, file)\n",
    "# #                     dcm_data = dcmread(dcm_file_path)\n",
    "# #                     data.append(\n",
    "# #                         {\n",
    "# #                             \"patient\": dcm_data.PatientName,\n",
    "# #                             \"exam\": dcm_data.StudyDescription,\n",
    "# #                             \"file_path\": dcm_file_path,\n",
    "# #                         }\n",
    "# #                     )\n",
    "# #                 except Exception as e:\n",
    "# #                     st.warning(f\"Error reading DICOM file {file}: {e}\")\n",
    "\n",
    "# #     return pd.DataFrame(data)\n",
    "\n",
    "# # start_folder = \"/volumes/cm7/start_folder\"\n",
    "\n",
    "# # if os.path.exists(start_folder) and os.path.isdir(start_folder):\n",
    "# #     folder = st.sidebar.selectbox(\"Select a folder:\", os.listdir(start_folder), index=0)\n",
    "# #     selected_folder = os.path.join(start_folder, folder)\n",
    "\n",
    "# #     if os.path.exists(selected_folder) and os.path.isdir(selected_folder):\n",
    "# #         dicom_df = load_dicom_data(selected_folder)\n",
    "\n",
    "# #         if not dicom_df.empty:\n",
    "# #             st.subheader(\"Available Studies\")\n",
    "# #             unique_studies = dicom_df[[\"patient\", \"exam\"]].drop_duplicates()\n",
    "# #             study_list = [f\"{row.patient} - {row.exam}\" for _, row in unique_studies.iterrows()]\n",
    "\n",
    "# #             selected_study = st.selectbox(\"Select a study:\", study_list)\n",
    "\n",
    "# #             selected_patient, selected_exam = selected_study.split(\" - \")\n",
    "\n",
    "# #             selected_images = dicom_df[\n",
    "# #                 (dicom_df[\"patient\"] == selected_patient) & (dicom_df[\"exam\"] == selected_exam)\n",
    "# #             ][\"file_path\"].tolist()\n",
    "\n",
    "# #             st.subheader(\"Selected Study Images\")\n",
    "# #             cols = st.columns(4)\n",
    "\n",
    "# #             for idx, image_file in enumerate(selected_images):\n",
    "# #                 with cols[idx % 4]:\n",
    "# #                     image = Image.open(image_file)\n",
    "# #                     st.image(image, caption=os.path.basename(image_file), use_column_width=True)\n",
    "# #         else:\n",
    "# #             st.warning(\"No DICOM files found in the folder.\")\n",
    "# # else:\n",
    "# #     st.error(\"Invalid start folder path.\")\n",
    "\n",
    "\n",
    "\n",
    "# # # def dir_selector(folder_path='/volumes/cm7/archived/modified/CmmDemoCase6/'):\n",
    "# # #     while True:\n",
    "# # #         folder_list = [f for f in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, f))]\n",
    "# # #         folder_list.insert(0, '..')  # Add a \"..\" option to go back up one level\n",
    "# # #         unique_key = 'dir_selector_' + folder_path.replace(os.path.sep, '_')\n",
    "# # #         selected_folder = st.sidebar.selectbox('Select a folder:', folder_list, index=0, key=unique_key)\n",
    "\n",
    "# # #         if selected_folder == '..':\n",
    "# # #             folder_path = os.path.dirname(folder_path)\n",
    "# # #         else:\n",
    "# # #             folder_path = os.path.join(folder_path, selected_folder)\n",
    "# # #             break\n",
    "\n",
    "# # #     return folder_path\n",
    "    \n",
    "# # # def plot_slice(vol, slice_ix):\n",
    "# # #     fig, ax = plt.subplots()\n",
    "# # #     plt.axis('off')\n",
    "# # #     selected_slice = vol[slice_ix, :, :]\n",
    "# # #     ax.imshow(selected_slice, origin='lower', cmap='gray')\n",
    "# # #     return fig\n",
    "    \n",
    "\n",
    "# # # st.sidebar.title('DieSitCom')\n",
    "# # # dirname = dir_selector()\n",
    "\n",
    "# # # if dirname is not None:\n",
    "# # #     try:\n",
    "# # #         reader = sitk.ImageSeriesReader()\n",
    "# # #         dicom_names = reader.GetGDCMSeriesFileNames(dirname)\n",
    "# # #         reader.SetFileNames(dicom_names)\n",
    "# # #         reader.LoadPrivateTagsOn()\n",
    "# # #         reader.MetaDataDictionaryArrayUpdateOn()\n",
    "# # #         data = reader.Execute()\n",
    "# # #         img = sitk.GetArrayViewFromImage(data)\n",
    "    \n",
    "# # #         n_slices = img.shape[0]\n",
    "# # #         slice_ix = st.sidebar.slider('Slice', 0, n_slices, int(n_slices/2))\n",
    "# # #         output = st.sidebar.radio('Output', ['Image', 'Metadata'], index=0)\n",
    "# # #         if output == 'Image':\n",
    "# # #             fig = plot_slice(img, slice_ix)\n",
    "# # #             plot = st.pyplot(fig)\n",
    "# # #         else:\n",
    "# # #             metadata = dict()\n",
    "# # #             for k in reader.GetMetaDataKeys(slice_ix):\n",
    "# #                 metadata[k] = reader.GetMetaData(slice_ix, k)\n",
    "# #             df = pd.DataFrame.from_dict(metadata, orient='index', columns=['Value'])\n",
    "# #             st.dataframe(df)\n",
    "# #     except RuntimeError:\n",
    "# #         st.text('This does not look like a DICOM folder!')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Backup of all of utils so i can clean up\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import pydicom\n",
    "# from datetime import datetime\n",
    "\n",
    "# from pydicom.dataset import Dataset as DcmDataset\n",
    "# from pydicom.tag import BaseTag as DcmTag\n",
    "# from pydicom.multival import MultiValue as DcmMultiValue\n",
    "# from pydicom.datadict import keyword_for_tag\n",
    "# import matplotlib.pyplot as plt\n",
    "# from pathlib import Path\n",
    "# from torch.optim import lr_scheduler\n",
    "# import torch.backends.cudnn as cudnn\n",
    "# import torchvision\n",
    "# from torchvision import datasets, models, transforms\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from torchvision.transforms import ToTensor\n",
    "# import os, sys, glob, re\n",
    "# from joblib import dump, load\n",
    "# from fastai.basics import delegates\n",
    "# from fastcore.parallel import parallel\n",
    "# from fastcore.utils import gt\n",
    "# from fastcore.foundation import L\n",
    "# import sklearn\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, recall_score, ConfusionMatrixDisplay\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# ### local imports ###\n",
    "# from config import file_dict, abd_label_dict, classes, column_lists, feats\n",
    "# from config import val_list, train_val_split_percent, random_seed, data_transforms\n",
    "# from config import sentence_encoder, series_description_column\n",
    "# from sklearn.preprocessing import MultiLabelBinarizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# ### gets the dicom files from a provided directory ###\n",
    "# def get_dicoms(path, first_dcm=False, **kwargs):\n",
    "#     \"Walk `path` to get DICOM file names from specific extensions, then read files into a `pandas.DataFrame`. If `first_dcm=True`, only read first file from each folder.\"\n",
    "#     fns = L()\n",
    "#     extension_list=['.dcm','.dicom','.dcim','.ima']\n",
    "#     print(\"Finding DICOM files. This may take a few minutes.\")\n",
    "#     if first_dcm:\n",
    "#         for r, d, f in os.walk(path):\n",
    "#             if f:\n",
    "#                 if Path(f[0]).suffix.lower() in extension_list:\n",
    "#                     fns.append(Path(f'{r}/{f[0]}'))\n",
    "#     else:\n",
    "#         fns = L()\n",
    "#         for r, d, fs in os.walk(path):\n",
    "#             for f in fs:\n",
    "#                 if Path(f).suffix.lower() in extension_list:\n",
    "#                     fns.append(Path(f'{r}/{f}'))\n",
    "#     print(\"Reading DICOM files with extensions .dcm, .dicom, .dcim, or .ima. This may take a few minutes, depending on the number of files to read...\")\n",
    "#     df = pd.DataFrame.from_dicoms(fns, **kwargs)\n",
    "#     return fns, df\n",
    "\n",
    "# ### Reads a DICOM file and returns the corresponding pydicom.Dataset ###\n",
    "# def dcmread(fn: Path, no_pixels=True, force=True):\n",
    "#     return pydicom.dcmread(str(fn), stop_before_pixels=no_pixels, force=force)\n",
    "\n",
    "# def cast_dicom_special(x):\n",
    "#     cls = type(x)\n",
    "#     if not cls.__module__.startswith('pydicom'): return x\n",
    "#     if cls.__base__ == object: return x\n",
    "#     return cls.__base__(x)\n",
    "\n",
    "# def split_elem(res, k, v):\n",
    "#     if not isinstance(v, DcmMultiValue): return\n",
    "#     for i, o in enumerate(v): res[f'{k}{\"\" if i == 0 else i}'] = o\n",
    "\n",
    "\n",
    "# def as_dict(self: DcmDataset, filt=True, split_multi=False):\n",
    "#     if filt:\n",
    "#         vals = [self[o] for o in self.keys() if self[o].keyword in column_lists['dicom_cols']]\n",
    "        \n",
    "#     else:\n",
    "#         vals = [self[o] for o in self.keys()]\n",
    "#     items = [(v.keyword, v.value.name) if v.keyword == 'SOPClassUID' else (v.keyword, v.value) for v in vals]\n",
    "#     res = dict(items)\n",
    "#     res['fname'] = self.filename\n",
    "#     if split_multi:\n",
    "#         for k, v in items: split_elem(res, k, v)\n",
    "#         for k in res: res[k] = cast_dicom_special(res[k])\n",
    "#     return res\n",
    "# DcmDataset.as_dict = as_dict\n",
    "\n",
    "# # def _dcm2dict(fn, excl_private=False, **kwargs):\n",
    "# #     ds = fn.dcmread(**kwargs)\n",
    "# #     if excl_private: ds.remove_private_tags()\n",
    "# #     return ds.as_dict(**kwargs)\n",
    "\n",
    "# def dcm2dict(fn, excl_private=False, **kwargs):\n",
    "#     ds = dcmread(fn, **kwargs)\n",
    "#     if excl_private: ds.remove_private_tags()\n",
    "#     return ds.as_dict(**kwargs)\n",
    "\n",
    "# # def dcm2dict2(dcm_file, excl_private = False, **kwargs):\n",
    "# #     dcm_data = dcmread(dcm_file, **kwargs)\n",
    "# #     if excl_private: dcm_data.remove_private_tags()\n",
    "# #     dcm_dict = {keyword_for_tag(tag): dcm_data.get(tag) for tag in dcm_data.keys()}\n",
    "# #     return dcm_dict\n",
    "\n",
    "\n",
    "\n",
    "# @delegates(parallel)\n",
    "# def from_dicoms(cls, fns, n_workers=0, **kwargs):\n",
    "#     return pd.DataFrame(parallel(dcm2dict, fns, n_workers=n_workers, **kwargs))\n",
    "# pd.DataFrame.from_dicoms = classmethod(from_dicoms)\n",
    "\n",
    "# # def _from_dicoms(cls, fns):\n",
    "# #     dicts = [dcm2dict(fn) for fn in fns]  # Process the files sequentially\n",
    "# #     return pd.DataFrame(dicts)\n",
    "# # pd.DataFrame.from_dicoms = classmethod(_from_dicoms)\n",
    "\n",
    "# def get_series_fp(fn): return Path(fn).parent\n",
    "\n",
    "#  ### takes the contents of the dataframe column with path/filenames and converts pieces into separate df columns ###   \n",
    "# def expand_filename_into_columns(df, cols):\n",
    "#     for iterator in range(len(cols)):\n",
    "#         df[cols[iterator]] = df['fname'].astype(str).apply(lambda x: x.split('/')[iterator])\n",
    "#     return df\n",
    "\n",
    "# ### another version which goes backwards from the end of the filename\n",
    "# def expand_filename(df, cols):\n",
    "#     for iterator in range(len(cols)):\n",
    "#         df[cols[iterator]] = df['fname'].astype(str).apply(lambda x: x.split('/')[-iterator])\n",
    "#     return df\n",
    "\n",
    "# def extract_image_number(filename):\n",
    "#     pattern = r'-([0-9]+)\\.dcm$'\n",
    "#     match = re.search(pattern, filename)\n",
    "#     if match:\n",
    "#         return int(match.group(1))\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "\n",
    "# def compute_plane(row):\n",
    "#     '''\n",
    "#     Computes the plane of imaging from the direction cosines provided in the `ImageOrientationPatient` field.\n",
    "#     The format of the values in this field is: `[x1, y1, z1, x2, y2, z2]`,\n",
    "#     which correspond to the direction cosines for the first row and column of the image pixel data.\n",
    "#     '''\n",
    "#     planes = ['sag', 'cor', 'ax']\n",
    "#     if 'ImageOrientationPatient1' in row.keys():\n",
    "#         dircos = [v for k, v in row.items() if 'ImageOrientationPatient' in k]\n",
    "#     else:\n",
    "#         dircos = row['ImageOrientationPatient']\n",
    "\n",
    "#         # Handle MultiValue objects by converting them to a list of floats\n",
    "#         if isinstance(dircos, DcmMultiValue):\n",
    "#             dircos = [float(x) for x in dircos]\n",
    "\n",
    "#     # Check if dircos has the expected length\n",
    "#     if not isinstance(dircos, float) and len(dircos) == 6:\n",
    "#         dircos = np.array(dircos).reshape(2, 3)\n",
    "#         pnorm = abs(np.cross(dircos[0], dircos[1]))\n",
    "#         return planes[np.argmax(pnorm)]\n",
    "#     else:\n",
    "#         return 'unknown'\n",
    "\n",
    "# #_re_extra_info = re.compile(r'[<\\([].*?[\\]\\)>]')\n",
    "\n",
    "# def rm_extra_info(t):\n",
    "#     \"Remove extraneous info in closures\"\n",
    "#     return re.compile(r'[<\\([].*?[\\]\\)>]').sub('', t).strip()\n",
    "\n",
    "\n",
    "# def detect_contrast(row):\n",
    "    \n",
    "#     #if entry in contrastbolusagent contains a value\n",
    "#     try:\n",
    "#         c = row['ContrastBolusAgent']\n",
    "#         if type(c) == str: return 1\n",
    "#     except KeyError:\n",
    "#         pass\n",
    "#     # heuristic based on series description text\n",
    "#     sd = rm_extra_info(str(row['SeriesDescription']).lower())\n",
    "#     _c = re.compile(r'(\\+-?c|post|with|dyn|portal|equilibrium|hepatobiliary|delayed)')\n",
    "#     if _c.search(sd): return 1\n",
    "\n",
    "#     return 0\n",
    "\n",
    "\n",
    "\n",
    "# def _find_seq(sd):\n",
    "#     if _t1.search(sd):\n",
    "#         if _spgr.search(sd): return 'spgr'\n",
    "#         if _t1_in.search(sd): return 'in phase'\n",
    "#         if _t1_out.search(sd): return 'opposed phase'\n",
    "#         if _water.search(sd): return 'dixon water'\n",
    "#         if _fat.search(sd): return 'dixon fat'    \n",
    "#         if _pv.search(sd): return 'portal venous'\n",
    "#         if _eq.search(sd): return 'equilibrium'\n",
    "#         if _art.search(sd): return 'early dynamic'\n",
    "#         if _delayed.search(sd): return 'hepatobiliary'\n",
    "#         else: return 't1'\n",
    "#     if _t1_in.search(sd): return 'in phase'\n",
    "#     if _t1_out.search(sd): return 'opposed phase'\n",
    "#     if _water.search(sd): return 'dixon water'\n",
    "#     if _fat.search(sd): return 'dixon fat'    \n",
    "#     if _pv.search(sd): return 'portal venous'\n",
    "#     if _eq.search(sd): return 'equilibrium'\n",
    "#     if _art.search(sd): return 'early dynamic'\n",
    "#     if _delayed.search(sd): return 'hepatobiliary'\n",
    "# #    if _spgr.search(sd): return 'spgr'\n",
    "#     if _t2.search(sd):\n",
    "#         if _flair.search(sd): return 'flair'\n",
    "#         elif _swi.search(sd): return 'swi'\n",
    "#         else: return 't2'\n",
    "#     if _flair.search(sd): return 'flair'\n",
    "#     if _swi.search(sd): return 'swi'\n",
    "#     if _dwi.search(sd): return 'dwi'\n",
    "#     if _adc.search(sd): return 'dwi'\n",
    "#     if _eadc.search(sd): return 'dwi'\n",
    "#     if _mra.search(sd): return 'mra'\n",
    "#     if _loc.search(sd): return 'loc'\n",
    "#     if _other.search(sd): return 'other'\n",
    "#     return 'unknown'\n",
    "\n",
    "\n",
    "\n",
    "# def _make_col_binary(df, col):\n",
    "#     s = df[col].isna()\n",
    "#     if any(s):\n",
    "#         df[col] = s.apply(lambda x: 0 if x else 1)\n",
    "#     else:\n",
    "#         targ = df.loc[0, col]\n",
    "#         df[col] = df[col].apply(lambda x: 0 if x == targ else 1)\n",
    "\n",
    "# def make_binary_cols(df, cols):\n",
    "#     df1 = df.copy()\n",
    "#     for col in cols:\n",
    "#         if col in df.columns:\n",
    "#             _make_col_binary(df1, col)\n",
    "#         else: \n",
    "#             df1[col]=0\n",
    "#             _make_col_binary(df1,col)\n",
    "#     return df1\n",
    "\n",
    "# def rescale_cols(df, cols):\n",
    "#     df1 = df.copy()\n",
    "#     scaler = MinMaxScaler()\n",
    "#     df1[cols] = scaler.fit_transform(df1[cols])\n",
    "#     return df1.fillna(0)\n",
    "\n",
    "# def get_dummies(df, cols=column_lists['dummies'], prefix=column_lists['d_prefixes']):\n",
    "#     df1 = df.copy()\n",
    "#     for i, col in enumerate(cols):\n",
    "#         df1[col] = df1[col].fillna('NONE')\n",
    "#         mlb = MultiLabelBinarizer()\n",
    "#         df1 = df1.join(\n",
    "#             pd.DataFrame(mlb.fit_transform(df1.pop(col)), columns=mlb.classes_).add_prefix(f'{prefix[i]}_')\n",
    "#         )\n",
    "#     return df1\n",
    "\n",
    "# #get labels from a text file in the format of comma deliminted four columns ()\n",
    "# def labels_from_file(label_path, column_names):\n",
    "\n",
    "#     label_df = pd.read_csv(label_path,header=None)\n",
    "#     label_df.columns=column_names\n",
    "\n",
    "#     return label_df\n",
    "\n",
    "# def preprocess(df, keep= column_lists['keep'], dummies= column_lists['dummies'], d_prefixes= column_lists['d_prefixes'], binarize= column_lists['binarize'], rescale= column_lists['rescale']):\n",
    "#    #Preprocess metadata for Random Forest classifier to predict sequence type\n",
    "#     print(\"Preprocessing metadata for Random Forest classifier.\")\n",
    "#     df1 = exclude_other(df)\n",
    "#     print(f\"Have received {df1.shape[0]} entries.\")\n",
    "    \n",
    "#     # Only keep columns that are both in the DataFrame and the 'keep' list\n",
    "#     df1 = df1[[col for col in keep if col in df1.columns]]\n",
    "    \n",
    "#     if 'PixelSpacing' in df1.columns and df1['PixelSpacing'].any:\n",
    "#         df1['PixelSpacing'] = df1['PixelSpacing'].apply(lambda x: x[0])\n",
    "    \n",
    "#     # Only get dummies for columns that are in the DataFrame\n",
    "#     dummies = [col for col in dummies if col in df1.columns]\n",
    "#     df1 = get_dummies(df1, dummies, d_prefixes)\n",
    "    \n",
    "#     # Only make binary columns for columns that are in the DataFrame\n",
    "#     binarize = [col for col in binarize if col in df1.columns]\n",
    "#     df1 = make_binary_cols(df1, binarize)\n",
    "    \n",
    "#     # Only rescale columns that are in the DataFrame\n",
    "#     rescale = [col for col in rescale if col in df1.columns]\n",
    "#     df1 = rescale_cols(df1, rescale)\n",
    "    \n",
    "#     for f in feats:\n",
    "#         if f not in df1.columns:\n",
    "#             df1[f] = 0\n",
    "            \n",
    "#     return df1\n",
    "\n",
    "\n",
    "\n",
    "# # def preprocess2(df, keep=column_lists['keep'], dummies=column_lists['dummies'], d_prefixes=column_lists['d_prefixes'], binarize=column_lists['binarize'], rescale=column_lists['rescale']):\n",
    "# #     \"Preprocess metadata for Random Forest classifier to predict sequence type\"\n",
    "# #     print(\"Preprocessing metadata for Random Forest classifier.\")\n",
    "# #     df1 = exclude_other(df)\n",
    "# #     print(f\"Have received {df1.shape[0]} entries.\")\n",
    "    \n",
    "# #     # Only keep columns that are both in the DataFrame and the 'keep' list\n",
    "# #     df1 = df1[[col for col in keep if col in df1.columns]]\n",
    "    \n",
    "# #     if 'PixelSpacing' in df1.columns and df1['PixelSpacing'].any:\n",
    "# #         df1['PixelSpacing'] = df1['PixelSpacing'].apply(lambda x: x[0])\n",
    "    \n",
    "# #     # Only get dummies for columns that are in the DataFrame\n",
    "# #     dummies = [col for col in dummies if col in df1.columns]\n",
    "# #     df1 = get_dummies(df1, dummies, d_prefixes)\n",
    "    \n",
    "# #     # Only make binary columns for columns that are in the DataFrame\n",
    "# #     binarize = [col for col in binarize if col in df1.columns]\n",
    "# #     df1 = make_binary_cols(df1, binarize)\n",
    "    \n",
    "# #     # Only rescale columns that are in the DataFrame\n",
    "# #     rescale = [col for col in rescale if col in df1.columns]\n",
    "# #     df1 = rescale_cols(df1, rescale)\n",
    "    \n",
    "# #     for f in feats:\n",
    "# #         if f not in df1.columns:\n",
    "# #             df1[f] = 0\n",
    "            \n",
    "# #     return df1\n",
    "\n",
    "\n",
    "# def convert_labels_from_file(label_df):\n",
    "#     labels=label_df.copy()\n",
    "#     labels['GT label'] = labels['label_code'].astype(str).apply(lambda x: abd_label_dict[x]['short'])\n",
    "#     labels['GT plane'] = labels['label_code'].astype(str).apply(lambda x: abd_label_dict[x]['plane'])\n",
    "#     labels['GT contrast'] = labels['label_code'].astype(str).apply(lambda x: abd_label_dict[x]['contrast'])\n",
    "#     labels['patientID'] = labels['patientID'].astype(str)\n",
    "# #    labels['Parent_folder'] = labels['fname'].astype(str).apply(lambda x: x.split('/')[0])\n",
    "# #    labels['patientID'] = labels['fname'].astype(str).apply(lambda x: x.split('/')[1]).astype(int)\n",
    "# #    labels['exam'] = labels['fname'].astype(str).apply(lambda x: x.split('/')[2])\n",
    "# #    labels['series'] = labels['fname'].astype(str).apply(lambda x: x.split('/')[3])\n",
    "    \n",
    "#     return labels\n",
    "\n",
    "# def expand_filename_into_columns(df, cols):\n",
    "#     for iterator in range(len(cols)):\n",
    "#         df[cols[iterator]] = df['fname'].astype(str).apply(lambda x: x.split('/')[iterator])\n",
    "#     return df\n",
    "\n",
    "# def train_setup(df, preproc=True):\n",
    "#     \"Extract labels for training data and return 'unknown' as test set\"\n",
    "#     if preproc:\n",
    "#         df1 = preprocess(df)\n",
    "#         labels = extract_labels(df1)\n",
    "#         df1 = df1.join(labels[['plane', 'contrast', 'seq_label']])\n",
    "#     else:\n",
    "#         df1 = df.copy()\n",
    "#     filt = df1['seq_label'] == 'unknown'\n",
    "#     train = df1[~filt].copy().reset_index(drop=True)\n",
    "#     test = df1[filt].copy().reset_index(drop=True)\n",
    "#     y, y_names = pd.factorize(train['seq_label'])\n",
    "#     return train, test, y, y_names\n",
    "\n",
    "# def train_setup_abdomen(df, cols=['patientID','exam','series'], preproc=False, need_labels=False):\n",
    "\n",
    "#     if preproc:\n",
    "#         df1=preprocess(df)\n",
    "        \n",
    "#     else:\n",
    "#         df1=df.copy()\n",
    "    \n",
    "#     if need_labels:\n",
    "\n",
    "#         labels = extract_labels(df1)\n",
    "#         df1 = df1.merge(labels, on=cols)\n",
    " \n",
    "#     length = df1.shape[0]\n",
    "\n",
    "#     #gkf = GroupKFold(n_splits=5)\n",
    "#     #for train_set, val_set in gkf.split(df1, groups=df1['patientID']):\n",
    "#     #    train, val = df1.loc[train_set], df1.loc[val_set]\n",
    "   \n",
    "#     train_set, val_set = next(GroupShuffleSplit(test_size=.20, n_splits=1, random_state = 42).split(df1, groups=df1['patientID']))\n",
    "\n",
    "#     train = df1.iloc[train_set]\n",
    "#     val = df1.iloc[val_set]\n",
    "#     y, y_names = train['label_code'],train['GT label']\n",
    " \n",
    "#     return train, val, y, y_names\n",
    "\n",
    "# # def _get_meta_preds(clf, df, features, y_names=_y_names):\n",
    "# #     y_pred = clf.predict(df[features])\n",
    "# #     y_prob = clf.predict_proba(df[features])\n",
    "# #     preds = pd.Series(y_names.take(y_pred))\n",
    "# #     probas = pd.Series([y_prob[i][pred] for i, pred in enumerate(y_pred)])\n",
    "# #     return pd.DataFrame({'seq_pred': preds, 'pred_proba': probas})\n",
    "\n",
    "# # def predict_from_df(df, features=_features, thresh=0.8, model_path=_model_path, clf=None, **kwargs):\n",
    "# #     \"Predict series from `df[features]` at confidence threshold `p >= thresh`\"\n",
    "# #     if 'plane' not in df.columns:\n",
    "# #         df1 = preprocess(df)\n",
    "# #         labels = extract_labels(df1)\n",
    "# #         df1 = df1.join(labels[['plane', 'contrast', 'seq_label']])\n",
    "# #     else:\n",
    "# #         df1 = df.copy()\n",
    "# #     if clf:\n",
    "# #         model_path = None\n",
    "# #     else:\n",
    "# #         clf = load(model_path)    \n",
    "# #     df1 = df1.join(_get_preds(clf, df1, features, **kwargs))\n",
    "# #     filt = df1['pred_proba'] < thresh\n",
    "# #     df1['seq_pred'][filt] = 'unknown'\n",
    "# #     return df1\n",
    "\n",
    "# # def predict_from_folder(path, **kwargs):\n",
    "# #     \"Read DICOMs into a `pandas.DataFrame` from `path` then predict series\"\n",
    "# #     _, df = get_dicoms(path)\n",
    "# #     return predict_from_df(df, **kwargs)\n",
    "\n",
    "# # class Finder():\n",
    "# #     \"A class for finding DICOM files of a specified sequence type from a specific .\"\n",
    "# #     def __init__(self, path):\n",
    "# #         self.root = path\n",
    "# #         self.fns, self.dicoms = get_dicoms(self.root)\n",
    "# #         self.dicoms = preprocess(self.dicoms)\n",
    "# #         self.labels = extract_labels(self.dicoms)\n",
    "# #         self.dicoms = self.dicoms.join(self.labels[['plane', 'contrast']])\n",
    "        \n",
    "# #     def predict(self,  model_path=_model_path, features=_features, ynames=_y_names, **kwargs):\n",
    "# #         try:\n",
    "# #             self.clf = load(model_path)\n",
    "# #         except FileNotFoundError as e:\n",
    "# #             print(\"No model found. Try again by passing the `model_path` keyword argument.\")\n",
    "# #             raise\n",
    "# #         self.features = features\n",
    "# #         self.ynames = ynames\n",
    "# #         preds = self.clf.predict(self.dicoms[features])\n",
    "# #         self.preds = ynames.take(preds)\n",
    "# #         self.probas = self.clf.predict_proba(self.dicoms[features])\n",
    "        \n",
    "# #     def find(self, plane='ax', seq='t1', contrast=True, thresh=0.8, **kwargs):\n",
    "# #         try:\n",
    "# #             self.probas\n",
    "# #         except AttributeError:\n",
    "# #             print(\"Prediction not yet performed. Please run `Finder.predict()` and try again.\")\n",
    "# #             raise\n",
    "# #         preds = np.argwhere(self.probas > 0.8)\n",
    "# #         ind = preds[:, 0]\n",
    "# #         pred_names = _y_names.take(preds[:, 1])\n",
    "# #         df = pd.DataFrame(pred_names, index=ind, columns=['seq_pred'])\n",
    "# #         df = self.dicoms[_output_columns].join(df)\n",
    "# #         return df.query(f'plane == \"{plane}\" and seq_pred == \"{seq}\" and contrast == {int(contrast)}')\n",
    "    \n",
    "\n",
    "\n",
    "# def exclude_other(df):\n",
    "#     if 'BodyPartExamined' not in df.columns: return df\n",
    "#     other = ['SPINE', 'CSPINE', 'PELVIS', 'PROSTATE']\n",
    "#     filt = df.BodyPartExamined.isin(other)\n",
    "#     df1 = df[~filt].copy().reset_index(drop=True)\n",
    "#     filt1 = df1.SeriesDescription.str.contains(r'(cervical|thoracic|lumbar)', case=False, na=False)\n",
    "#     df2 = df1[~filt1].reset_index(drop=True)\n",
    "#     filt2 = df2.SOPClassUID == \"MR Image Storage\"\n",
    "#     return df2[filt2].reset_index(drop=True)  \n",
    "\n",
    "# def load_pickled_dataset(train_file, test_file):\n",
    "#   with open(train_file, 'rb') as f:\n",
    "#     train_df = pickle.load(f)\n",
    "#   with open(test_file, 'rb') as g:\n",
    "#     test_df = pickle.load(g)\n",
    "\n",
    "#   return train_df, test_df\n",
    "\n",
    "# def create_val_dataset_from_csv(train_file, test_file, val=True, val_lists=val_list):\n",
    "#     train_df = pd.read_csv(train_file)\n",
    "#     test_df = pd.read_csv(test_file)\n",
    "    \n",
    "#     if val:\n",
    "#         if val_lists:\n",
    "#             val_df = train_df[train_df.patientID.isin(val_lists)]\n",
    "#             train_df = train_df[~train_df.index.isin(val_df.index)] \n",
    "#         else:\n",
    "#             train_set, val_set = next(GroupShuffleSplit(test_size=.20, n_splits=1, random_state = 42).split(train_df, groups=train_df['patientID']))\n",
    "#             train_df, val_df = train_set, val_set\n",
    "#         return train_df, val_df, test_df\n",
    "\n",
    "#     else: \n",
    "#         return train_df, test_df\n",
    "\n",
    "\n",
    "# def load_datasets_from_csv(train_csv, val_csv, test_csv):\n",
    "#     train_df = pd.read_csv(train_csv)\n",
    "#     val_df = pd.read_csv(val_csv)\n",
    "#     test_df = pd.read_csv(test_csv)\n",
    "\n",
    "#     return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "# # this function will select the image in the middle of each series of images, so that only a single image from each series is selected for training\n",
    "# # There is one image from each series for each patient\n",
    "# def shorten_df(df, selection_fraction = 0.5):\n",
    "#   df1 = df.copy()\n",
    "#   grouped_df = df.groupby(['patientID', 'series'])\n",
    "#   sorted_df = grouped_df['file_info'].apply(lambda x: x.sort_values())\n",
    "  \n",
    "#   selected_filename = grouped_df['file_info'].apply(lambda x: x.sort_values().iloc[int(len(x)*selection_fraction)])\n",
    "\n",
    "  \n",
    "#   selected_filename = selected_filename.reset_index()\n",
    "  \n",
    "#   # perform merge and deal with duplicate/unnecessary columns\n",
    "#   df1 = df1.merge(selected_filename, on=['patientID', 'series'], how='left') \n",
    "#   df_short = df1.drop(['file_info_x', 'img_num'], axis=1)\n",
    "#   df_short = df_short.rename(columns = {'file_info_y': 'file_info'})\n",
    "#   df_short.drop_duplicates(inplace=True)\n",
    "#   df_short.reset_index(drop=True, inplace=True)\n",
    "#   return df_short\n",
    "\n",
    "# # like shorten_df but does not adjust the dataframe, just returns the selected filenames\n",
    "# def mask_one_from_series(df, selection_fraction=0.5):\n",
    "#     df1 = df.copy()\n",
    "#     grouped_df = df.groupby(['patientID', 'series'])\n",
    "#     sorted_df = grouped_df['file_info'].apply(lambda x: x.sort_values())\n",
    "  \n",
    "#     selected_rows = grouped_df['file_info'].apply(lambda x: x.sort_values().iloc[int(len(x)*selection_fraction)])\n",
    "   \n",
    "#     return selected_rows\n",
    "\n",
    "# def prepare_df(df):\n",
    "#     df1 = df.copy()\n",
    "#     filenames = df1.file_info.tolist()\n",
    "#     getdicoms = pd.DataFrame.from_dicoms(filenames)\n",
    "#     merged = getdicoms.merge(df1, left_on='fname', right_on='file_info')\n",
    "#     merged.drop(columns=['file_info'], inplace=True)\n",
    "    \n",
    "#     merged['contrast'] = merged.apply(detect_contrast, axis=1)\n",
    "#     merged['plane'] = merged.apply(compute_plane, axis=1)\n",
    "    \n",
    "    \n",
    "#     return merged\n",
    "\n",
    "\n",
    "\n",
    "# def pool_arterial_labels(df, label_col='label'):\n",
    "#     df1 = df.copy()\n",
    "#     df1[label_col] = df1[label_col].apply(lambda x: 2 if x in [2,3,4,5] else x)\n",
    "#     return df1\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "# #visualization of a batch of images\n",
    "# def imshow(img, title):\n",
    "#     img = torchvision.utils.make_grid(img, normalize=True)\n",
    "#     npimg = img.numpy()\n",
    "#     fig = plt.figure(figsize = (5, 15))\n",
    "#     plt.imshow(np.transpose(npimg,(1,2,0)))\n",
    "#     plt.title(title)\n",
    "\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "\n",
    "# # produce high quality confusion matrix image and save it\n",
    "# # still need to add labels\n",
    "# def plot_and_save_cm(ytrue, ypreds, fname):\n",
    "#     cm = confusion_matrix(ytrue, ypreds)\n",
    "#     plt.figure(figsize=(25, 25))\n",
    "#     plt.tight_layout()\n",
    "#     ConfusionMatrixDisplay(cm).plot(cmap='Blues')\n",
    "#     plt.savefig(fname + datetime.now().strftime('%Y%m%d') + \".png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "#     return plt\n",
    "\n",
    "\n",
    "# #explore train.fit and kgroupcross validation\n",
    "# def train_setup_abdomen_cross(df, cols=['patientID','exam','series'], need_preproc=False, need_labels=False):\n",
    "\n",
    "#     if need_preproc:\n",
    "#         df1=preprocess(df)\n",
    "        \n",
    "#     else:\n",
    "#         df1=df.copy()\n",
    "    \n",
    "#     if need_labels:\n",
    "\n",
    "#         labels = extract_labels(df1)\n",
    "#         df1 = df1.merge(labels, on=cols)\n",
    " \n",
    "#     length = df1.shape[0]\n",
    "\n",
    "#     gkf = GroupKFold(n_splits=5)\n",
    "#     for train_set, val_set in gkf.split(df1, groups=df1['patientID']):\n",
    "#         #print(train_set, len(train_set), train_set.dtype)\n",
    "#         #print(df1.iloc[train_set])\n",
    "#         train_df, val = df1.iloc[train_set], df1.iloc[val_set]\n",
    "#         y, y_names = train_df['label_code'],train_df['GT label']\n",
    "        \n",
    "#         clf_gkf = train_fit(train_df, y, features=preproc._features, fname='cross_from_notebook.skl' )\n",
    "#         scores = cross_validate(clf_gkf, train_df[preproc._features], y, scoring=['precision_macro', 'recall_macro'])\n",
    "#         print(scores)\n",
    "#     #return train, val, y, y_names\n",
    "\n",
    "# # adds the preds and probs to select rows from the original data frame (patient and study info)\n",
    "# def make_results_df(preds, probs, true, df):\n",
    "#     return pd.DataFrame({'preds': preds, 'true': true, 'probs': [row.tolist() for row in probs], 'patientID': df['patientID'], 'series_description': df['SeriesDescription'], 'contrast': df['contrast'], 'plane': df['plane']  })\n",
    "\n",
    "\n",
    "    \n",
    "# def display_and_save_results(y_pred, y_true, classes=classes, fn='', saveflag = True):\n",
    "   \n",
    "#     class_text_labels = [abd_label_dict[str(x)]['short'] for x in classes]\n",
    "   \n",
    "#      # Generate a classification report based on the true labels and predicted labels\n",
    "#     print(classification_report(y_true, y_pred))\n",
    "\n",
    "#     # Generate a confusion matrix based on the true labels and predicted labels\n",
    "#     cm = confusion_matrix(y_true = y_true, y_pred = y_pred, labels=classes)\n",
    "\n",
    "#     # Create a ConfusionMatrixDisplay object with the correct labels\n",
    "#     cm_display = ConfusionMatrixDisplay(cm, display_labels=class_text_labels).plot(xticks_rotation = 'vertical', cmap='Blues')\n",
    "#     plt.figure(figsize=(25, 25))\n",
    "#     plt.tight_layout()\n",
    "#     #ConfusionMatrixDisplay(cm, display_labels=class_text_labels).plot(xticks_rotation = 'vertical', cmap='Blues')\n",
    "#     if saveflag:\n",
    "#         plt.savefig(\"../assets/FigCM_\"+fn+datetime.today().strftime('%Y%m%d')+\".tif\",dpi=300, bbox_inches = 'tight')     \n",
    "\n",
    "#     return cm      \n",
    "\n",
    "# def create_datasets(train_datafile, val_datafile, test_datafile):\n",
    "#     # reads in the dataframes from csv\n",
    "#     train_full = pd.read_csv(train_datafile)\n",
    "#     val_full = pd.read_csv(val_datafile)\n",
    "#     test_full = pd.read_csv(test_datafile)\n",
    "\n",
    "#     # selects the middle image from each series for further evaluation\n",
    "#     train = shorten_df(train_full)\n",
    "#     val = shorten_df(val_full)\n",
    "#     test = shorten_df(test_full)\n",
    "\n",
    "#     # changes to the dataframe including adding contrast and computed plane columns\n",
    "#     train_df = prepare_df(train)\n",
    "#     val_df = prepare_df(val)\n",
    "#     test_df = prepare_df(test)\n",
    "\n",
    "#     return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "# # #grid search for hyperparameters for the metadata model\n",
    "# # def train_fit_parameter_trial(train, y, features, fname='model-run.skl'):\n",
    "# #     \"Train a Random Forest classifier on `train[features]` and `y`, then save to `fname` and return.\"\n",
    "# #     clf = RandomForestClassifier(n_jobs=2, random_state=0)\n",
    "# #     clf.fit(train[features], y)\n",
    "# #     print('Parameters currently in use:\\n')\n",
    "# #     pprint(clf.get_params())\n",
    "    \n",
    "    \n",
    "# #     # Number of trees in random forest\n",
    "# #     n_estimators = [int(x) for x in np.linspace(start = 20, stop = 500, num = 20)]\n",
    "# #     # Number of features to consider at every split\n",
    "# #     max_features = ['auto', 'sqrt']\n",
    "# #     # Maximum number of levels in tree\n",
    "# #     max_depth = [int(x) for x in np.linspace(10, 660, num = 10)]\n",
    "# #     max_depth.append(None)\n",
    "# #     # Minimum number of samples required to split a node\n",
    "# #     min_samples_split = [2, 5, 10, 20]\n",
    "# #     # Minimum number of samples required at each leaf node\n",
    "# #     min_samples_leaf = [2, 4, 8]\n",
    "# #     # Method of selecting samples for training each tree\n",
    "# #     bootstrap = [True, False]\n",
    "# #     random_grid = {'n_estimators': n_estimators,\n",
    "# #                'max_features': max_features,\n",
    "# #                'max_depth': max_depth,\n",
    "# #                'min_samples_split': min_samples_split,\n",
    "# #                'min_samples_leaf': min_samples_leaf,\n",
    "# #                'bootstrap': bootstrap}\n",
    "    \n",
    "# #     clf_random = RandomizedSearchCV(estimator = clf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=0, n_jobs = -1)\n",
    "# #     clf_random.fit(train[features], y)\n",
    "# #     opt_clf = clf_random.best_estimator_\n",
    "# #     pprint(clf_random.best_params_)\n",
    "# #     pickle.dump(opt_clf, open(fname, 'wb'))\n",
    "# #     #dump(clf_random, fname)\n",
    "# #     return opt_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### updating the saved info for the fusion model \n",
    "with open('../data/fusion_train.pkl', 'rb') as file:\n",
    "    fusion_train = pickle.load(file)\n",
    "\n",
    "with open('../data/fusion_val.pkl', 'rb') as file:\n",
    "    fusion_val = pickle.load(file)\n",
    "\n",
    "with open('../data/fusion_test.pkl', 'rb') as file:\n",
    "    fusion_test = pickle.load(file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_train.meta_preds = meta_train_results_df.preds\n",
    "fusion_train.meta_probs = meta_train_results_df.probs\n",
    "fusion_val.meta_preds = meta_val_results_df.preds\n",
    "fusion_val.meta_probs = meta_val_results_df.probs\n",
    "fusion_test.meta_preds = meta_test_results_df.preds\n",
    "fusion_test.meta_probs = meta_test_results_df.probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resaving the datasets for the fusion :\n",
    "fusion_train.to_pickle('../data/fusion_train.pkl')\n",
    "fusion_val.to_pickle('../data/fusion_val.pkl')\n",
    "fusion_test.to_pickle('../data/fusion_test.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Instantiate your model\n",
    "model_container = ModelContainer()\n",
    "Fmodel = FusionModel(model_container)\n",
    "Fmodel_no_nlp = FusionModel(model_container, include_nlp=False)\n",
    "\n",
    "# Load the saved state_dict (model weights)\n",
    "saved_model_weights = torch.load('../models/fusion_model_weights042223.pth')\n",
    "saved_model_weights_no_nlp = torch.load('../models/fusion_model_weights_no_nlp042223.pth')\n",
    "\n",
    "# Load the state_dict into the model\n",
    "Fmodel.load_state_dict(saved_model_weights)\n",
    "Fmodel_no_nlp.load_state_dict(saved_model_weights_no_nlp)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "Fmodel.eval()\n",
    "\n",
    "predictions = []\n",
    "probs = []\n",
    "\n",
    "predictions_nn = []\n",
    "probs_nn = []\n",
    "\n",
    "for index, row in X_test_meta.iterrows():\n",
    "    pred, prob, ts_df = Fmodel.get_fusion_inference(row)\n",
    "    predictions.append(pred)\n",
    "    probs.append(prob)\n",
    "    display(ts_df)\n",
    "    \n",
    "    pred_nn, prob_nn, ts_df_nn = Fmodel_no_nlp.get_fusion_inference(row)\n",
    "    predictions_nn.append(pred_nn)\n",
    "    probs_nn.append(prob_nn)\n",
    "    display(ts_df_nn)\n",
    "\n",
    "comparison_fusion_df = pd.DataFrame({'preds': predictions, 'probs': probs, 'preds_no_nlp': predictions_nn, 'probs_no_nlp': probs_nn, 'true': TEy})\n",
    "comparison_fusion_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def get_fusion_inference(row, models = model_container_instance, device=device,features=feats_to_keep, num_classes=len(classes)):\n",
    "#     #meta_data, pixel_data, text_data = extract_data_for_models(row)\n",
    "    \n",
    "#     pred1, prob1 = get_meta_inference(row, model_container_instance.metadata_model, features) \n",
    "#     prob1_tensor = torch.tensor(prob1, dtype=torch.float32).squeeze()\n",
    "#     #print(f'shape of prob1_tensor is {prob1_tensor.shape}')\n",
    "    \n",
    "#     pred2, prob2 = pixel_inference(model_container_instance.cnn_model, [row.fname], classes=classes)\n",
    "#     prob2_tensor = torch.tensor(prob2, dtype=torch.float32)\n",
    "#     #print(f'shape of prob2_tensor is {prob2_tensor.shape}')\n",
    "    \n",
    "    \n",
    "#     pred3, prob3 = get_NLP_inference(model_container_instance.nlp_model, [row.fname], device, classes=classes)\n",
    "#     prob3_tensor = torch.tensor(prob3, dtype=torch.float32)\n",
    "#     #print(f'shape of prob3_tensor is {prob3_tensor.shape}')\n",
    "    \n",
    "#     # choose fusion model and instantiate\n",
    "#     fusion_model = model_container_instance.fusion_model\n",
    "\n",
    "#     # Pass the tensors through the FusionModel\n",
    "#     fused_output=  fusion_model(prob1_tensor, prob2_tensor, prob3_tensor)\n",
    "    \n",
    "#     # Get the predicted class and confidence score\n",
    "#     predicted_class = classes[torch.argmax(fused_output, dim=0).item()]\n",
    "#     confidence_score = torch.max(torch.softmax(fused_output, dim=0)).item()\n",
    "    \n",
    "#     return predicted_class, confidence_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MRI_project2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
